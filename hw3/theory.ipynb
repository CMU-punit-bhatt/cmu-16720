{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025d6187",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc25b0a4ba6f32aecb61b1c243e16de3",
     "grade": false,
     "grade_id": "cell-63d8d04a820a9a31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Neural Networks for Recognition - Assignment 3\n",
    "\n",
    "     Instructor: Kris Kitani                       TAs: Qichen(Lead), Paritosh, Rawal, Yan, Zen, Wen-Hsuan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e0b61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0598b3ff2a94d3ff38afa7b1c21e1c07",
     "grade": false,
     "grade_id": "cell-f9a9cc792e95e7c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Submission Instructions:\n",
    "\n",
    "1. Submit the PDF version of `theory.ipynb` to HW3:PDF. The `theory.ipynb` should include **ALL the writeup answers AND ALL the screenshots of code** specifically required in questions **from Q1 to Q7**. This section will be manually Graded.\n",
    "\n",
    "2. Submit `q2.ipynb`, `q3.ipynb`, `q5.ipynb` to HW3:Code. Please do not submit other jupyter notebooks as they will not be autograded. Submitting them may cause running time out. (`q5.ipynb` is optional for extra credits)\n",
    "\n",
    "**The Appendix section at the end of this file would help you on questions P1 and P2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee6744",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "544429557fd95d21890d2229c74a7ff4",
     "grade": false,
     "grade_id": "cell-e4414269f8c1645b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1 Theory Questions  (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab18847",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d8aa00056b3211954c375ea35e715b2",
     "grade": false,
     "grade_id": "cell-cf1227140b9ba295",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.1 (4 Points WriteUp)\n",
    "Prove that softmax is invariant to translation, that is \n",
    "$$softmax(x) = softmax(x + c) \\qquad \\forall c \\in \\mathbb{R}$$\n",
    "Softmax is defined as below, for each index $i$ in a vector $x$.\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
    "Often we use $c = − \\max x_i$. Why is that a good idea? (Tip: consider the range of values that numerator will have with $c = 0$ and $c = − \\max x_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22802e66",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3941e91facfc23f3fe531c8367d945e8",
     "grade": true,
     "grade_id": "cell-974a0ae8f06d6586",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Answer**\n",
    "\n",
    "To prove that $$softmax(x) = softmax(x + c) \\qquad \\forall c \\in \\mathbb{R}$$\n",
    "\n",
    "Consider $$softmax(x_i + c) = \\frac{e^{x_i + c}}{\\sum_j e^{x_j + c}} $$\n",
    "$$ = \\frac{e^{x_i}e^c}{e^{x_1}e^c + e^{x_2}e^c + \\cdots + e^{x_i}e^c + \\cdots }$$\n",
    "\n",
    "\n",
    "$$ = \\frac{e^ce^{x_i}}{e^c\\sum_j e^{x_j}} = \\frac{e^{x_i}}{\\sum_j e^{x_j}} = softmax(x_i)$$\n",
    "\n",
    "Using $c= -\\max x_i$ helps provide numerical stability. Exponential function grows very quickly as $x$ increases. This would also lead to the errors increasing exponentially and impact the performance of the model. Thus, reducing this exponential value helps provide stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b144ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e0ddcad64e1974a1b283ab4b960f3b8",
     "grade": false,
     "grade_id": "cell-948036aa5862be04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.2\n",
    "\n",
    "Softmax can be written as a three step processes, with $s_i = e^{x_i}$ , $S=\\sum_i s_i$ and $softmax(x_i)= \\frac{1}{S} s_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361cb29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c9340fc4070513afbb444b1d1563eed",
     "grade": false,
     "grade_id": "cell-52cff4a1493a7fff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.1 (1 point WriteUp)\n",
    "As $x \\in \\mathbb{R}^d$, what are the properties of $softmax(x)$, namely what is the range of each element? What is the sum over all elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83138ae7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67459c94158ef00d66c43c9896caf245",
     "grade": true,
     "grade_id": "cell-56521164bb7b5976",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In $softmax$, the exponential function, a monotonically increasing function, is used to handle all kinds of values - zeros, positive and negative. It then essentially takes the ratio of each element with respect to the total. Thus, the range of each element could be from 0 (exclusive) to 1 (inclusive). And the sum over all elements would lead to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f3cff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28faddaa495b54e962d322d74f319c7a",
     "grade": false,
     "grade_id": "cell-3a8e1905906ce40a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.2 (1 point WriteUp)\n",
    "One could say that ”softmax takes an arbitrary real valued vector $x$ and turns it into a ___”. Please think about a short phrase to fill in ___."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b6fa2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13459335a8f97ab284dfde9c24523418",
     "grade": true,
     "grade_id": "cell-6044a09dfc72a819",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Softmax takes an arbitrary real valued vector and turns it into a set of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec2b12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ac41c022a0f310b880438d66a81126e",
     "grade": false,
     "grade_id": "cell-501239c17b8d9fc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.3 (1 point WriteUp)\n",
    "Can you see the role of each step in the multi-step process now? Explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974cc8b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7279aaddce308bb0f897a9dd5e9d6ab",
     "grade": true,
     "grade_id": "cell-93d39e169a1584b1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In $softmax$, the exponential function, a monotonically increasing function, is used to handle all kinds of values - zeros, positive and negative, and convert them to positive real numbers. It then essentially takes the ratio of each element (after exp) with respect to the total (after exp). This ratio is always within 1 and the overall sum of each such ratio leads to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381cd7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f0139c980e6271bdcf924a4c4a09cca",
     "grade": false,
     "grade_id": "cell-182ef4991d435a9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.3 (3 points WriteUp)\n",
    "Show that multi-layer neural networks without a non-linear activation function are equivalent to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a3792",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "152725c4c687af9a3cbc311a9bf689e8",
     "grade": true,
     "grade_id": "cell-31c734ab09fd4344",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Consider a network with an input layer (i), a hidden layer (h) and an output layer (o).\n",
    "\n",
    "$\\therefore$ the pre activated outputs, $z_h = W_{hi}x + b_{hi}$$\n",
    "\n",
    "Consider a linear activation function such that, $a_h = kz_h$\n",
    "\n",
    "Similarly for output layer, preactivated outputs $$z_o = W_{oh}a_h  + b_{oh}$$\n",
    "\n",
    "And the post activated outputs would be $$a_o = kz_o$$\n",
    "\n",
    "$\\therefore a_o$ can be expanded as follows $$a_o = k\\left(W_{oh}a_h  + b_{oh}\\right)$$\n",
    "$$a_o = k\\left[W_{oh}k\\left(W_{hi}x + b_{hi}\\right)  + b_{oh}\\right]$$\n",
    "$$a_o = k^2W_{oh}W_{hi}x + \\left(k^2W_{oh}b_{hi} + b_{oh}\\right)$$\n",
    "\n",
    "This can be written as\n",
    "$$a_o = W`x + b`$$ where $W` = k^2W_{oh}W_{hi}$ and $b` = k^2W_{oh}b_{hi} + b_{oh}$\n",
    "\n",
    "As can be seen from above, the final output can be written as a linear combination of the input and some weights and are added with some constant. Thus, without a non-linear activation function, neural networks are equivalent to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce15f8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c65af3aee0fcba80878ceaa805e2c911",
     "grade": false,
     "grade_id": "cell-4e4a1870feae2779",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.4 (4 points WriteUp) \n",
    "Given the sigmoid activation function $\\sigma(x) = \\frac{1}{1+e^{-x}}$ , derive the gradient of the sigmoid function and show that it can be written as a function of $\\sigma(x)$ (without having access to $x$ directly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36bc4e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d7be6df91900e029bbd3c1e75e45be6",
     "grade": true,
     "grade_id": "cell-bb3ef4dae24f0472",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Given, $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "$$\\frac{d \\sigma(x)}{d x} = \\frac{d\\frac{1}{1+e^{-x}}}{d x}$$\n",
    "$$ = -\\frac{1}{(1 + e^{-x})^2}(-e^{-x})$$\n",
    "$$ = \\frac{1}{1 + e^{-x}} \\frac{e^{-x}}{1 + e^{-x}}$$\n",
    "$$ = \\frac{1}{1 + e^{-x}} \\left[\\frac{1 + e^{-x} - 1}{1 + e^{-x}}\\right]$$\n",
    "$$ = \\frac{1}{1 + e^{-x}} \\left[\\frac{1 + e^{-x}}{1 + e^{-x}} - \\frac{1}{1 + e^{-x}}\\right]$$\n",
    "$$ = \\frac{1}{1 + e^{-x}} \\left[1 - \\frac{1}{1 + e^{-x}}\\right]$$\n",
    "$$ = \\sigma(x) (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96ea82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3dff66146a32c6b75e3b0d8a1dc75740",
     "grade": false,
     "grade_id": "cell-c77fa30dd0533616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.5 (12 points WriteUp)\n",
    "\n",
    "Given $y = W x + b$ (or $y_j = \\sum_{i=1}^d  x_{i} W_{ji} + b_j$), and the gradient of some loss $J$ with respect $y$, show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$. Be sure to do the derivatives with scalars and re-form the matrix form afterwards. Here are some notional suggestions.\n",
    "$$ \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{k \\times 1} \\quad W \\in \\mathbb{R}^{k \\times d} \\quad x \\in \\mathbb{R}^{d \\times 1} \\quad b \\in \\mathbb{R}^{k \\times 1}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb05c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b66294c9dda4ab13c5d027e6bc82e64",
     "grade": true,
     "grade_id": "cell-4dbc318b383bf70c",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Finding derivate with respect to $W_{ji}$ element\n",
    "$$\\frac{\\partial J}{\\partial W_{ji}} = \\frac{\\partial J}{\\partial y_j}\\frac{\\partial y_J}{\\partial W_{ji}}$$\n",
    "$$\\frac{\\partial J}{\\partial W_{ji}} = \\delta_j\\frac{\\partial (\\sum_{z=1}^d  x_{z} W_{jz} + b_j)}{\\partial W_{ji}}$$\n",
    "$$\\frac{\\partial J}{\\partial W_{ji}} = \\delta_jx_i^T$$\n",
    "\n",
    "Summarizing this at the matrix level\n",
    "$$\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial W}$$\n",
    "$$\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial y}\\frac{\\partial (Wx + b)}{\\partial W}$$\n",
    "$$\\frac{\\partial J}{\\partial W} = \\delta x^T$$\n",
    "\n",
    "Finding derivate with respect to $b_j$ element\n",
    "$$\\frac{\\partial J}{\\partial b_j} = \\frac{\\partial J}{\\partial y_j}\\frac{\\partial y_J}{\\partial b_j}$$\n",
    "$$\\frac{\\partial J}{\\partial b_j} = \\delta_j\\frac{\\partial (\\sum_{z=1}^d  x_{z} W_{jz} + b_j)}{\\partial b_j}$$\n",
    "$$\\frac{\\partial J}{\\partial b_j} = \\delta_j$$\n",
    "\n",
    "Summarizing this at the matrix level\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial b}$$\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y}\\frac{\\partial (Wx + b)}{\\partial b}$$\n",
    "$$\\frac{\\partial J}{\\partial b} = \\delta$$\n",
    "\n",
    "Finding derivate with respect to $x_{ij}$ element\n",
    "$$\\frac{\\partial J}{\\partial x_{i}} = \\frac{\\partial J}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_{i}}$$\n",
    "$$\\frac{\\partial J}{\\partial x_{i}} = \\delta_j\\frac{\\partial (\\sum_{z=1}^d  x_{z} W_{jz} + b_u)}{\\partial x_{i}}$$\n",
    "$$\\frac{\\partial J}{\\partial x_{i}} = \\delta_{j} W_{ji}$$\n",
    "\n",
    "Summarizing this at the matrix level\n",
    "$$\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial b}$$\n",
    "$$\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}\\frac{\\partial (Wx + b)}{\\partial b}$$\n",
    "$$\\frac{\\partial J}{\\partial x} = W^T\\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ea3cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2815b900c97a10748f67524bf51b533a",
     "grade": false,
     "grade_id": "cell-920e213ee3ea40e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.6 (15 points WriteUp)\n",
    "\n",
    "We will find the derivatives for Conv layers now. Since most Deep Learning frameworks such as Pytorch, Tensorflow use cross-correlation in their respective \"convolution\" functions ([Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/convolution)), we will continue this abuse of notation. So the operation performed with the Conv Layer weights will be cross-correlation.\n",
    "    \n",
    "The input, $x$ is of shape $M\\times N$ with C channels. This will be *convolved* (actually cross-correlation) with $D$ number of $K\\times K$ filters, each with a bias term. The stride is 1 and there will be no padding. We know the gradient of some loss $J$ with respect to the output $y$, which will have $D$ channels. Show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$.\n",
    "\n",
    "The dimensions and notation are as follows:\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{D\\times M_o \\times N_o}\n",
    "    \\quad\n",
    "    M_o = M-K+1\n",
    "    \\quad\n",
    "    N_o = N-K+1\n",
    "$$\n",
    "$$\n",
    "    x \\in \\mathbb{R}^{C\\times M \\times N}\n",
    "    \\quad\n",
    "    W \\in \\mathbb{R}^{D\\times C \\times K \\times K}\n",
    "    \\quad\n",
    "    b \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "$x_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the input\n",
    "\n",
    "$y_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the output\n",
    "\n",
    "$W_{d, c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column, the $c^{th}$ channel of the kernel of the $d^{th}$ filter\n",
    "\n",
    "*For this question, you may compute the derivatives with scalars only. You don't need to re-form the matrix*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bc5ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d306d84ffe6763436b897502147ab18",
     "grade": true,
     "grade_id": "cell-84844fb51fed1aab",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Bias for channel d would affect all the $M_o x N_o$ output corresponding to the output channel d\n",
    "$$\\therefore \\frac{\\partial J}{\\partial b_d} = \\sum_{p=1}^{M_o}\\sum_{q=1}^{N_o} \\frac{\\partial J}{\\partial y_{d,p,q}} = \\sum_{p=1}^{M_o}\\sum_{q=1}^{N_o} \\delta_{d,p,q}$$\n",
    "\n",
    "Weight $W_{d,c,i,j}$ will affect all the $M_o x N_o$ corresponding to the output channel d. However, this weight would only activate input values in the cth channel and the i,j element for the window responsible for the current y output.\n",
    "\n",
    "$$\\therefore \\frac{\\partial J}{\\partial W_{d,c,i,j}} = \\sum_{p=1}^{M_o}\\sum_{q=1}^{N_o} \\frac{\\partial J}{\\partial y_{d,p,q}} x_{c, p + i, q + j} = \\sum_{p=1}^{M_o}\\sum_{q=1}^{N_o} \\delta_{d,p,q} x_{c, p + i, q + j}$$\n",
    "\n",
    "Input $x_{c,i,j}$ will affect every output and weight corresponding to each window it is a part of. Therefore, assuming we are dealing with an ideal input index (i, j), the element would be a part of $kxk$ windows. Each of these windows would correspond to 1 d channel output vector. Also, for each of these $kxk$ windows, the input would be combined with different weights in the cth channel.\n",
    "\n",
    "**Note** - Iterating from center of the windows. Hence, the $i - \\lceil\\frac{k}{2}\\rceil$ to $i + \\lceil\\frac{k}{2}\\rceil$ and $j - \\lceil\\frac{k}{2}\\rceil$ to $j + \\lceil\\frac{k}{2}\\rceil$\n",
    "\n",
    "$$\\therefore \\frac{\\partial J}{\\partial x_{c,i,j}} = \\sum_{p=max(i-\\lceil\\frac{k}{2}\\rceil, 0)}^{min(i + \\lceil\\frac{k}{2}\\rceil, M)}\\sum_{q=max(j-\\lceil\\frac{k}{2}\\rceil, 0)}^{min(j + \\lceil\\frac{k}{2}\\rceil, N)}\\sum_{r=1}^d \\delta_{r,p,q} W_{r, c, p - i, q - j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fabf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "674f5af48cfacd5f2b47e7085dba8ae3",
     "grade": false,
     "grade_id": "cell-1e10040668b36463",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.7\n",
    "\n",
    "When the neural network applies the elementwise activation function (such as sigmoid), the gradient of the activation function scales the back-propagation update. This is directly from the chain rule, $\\frac{d}{d x} f(g(x)) = f'(g(x)) g'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275b3c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23e18a4eafbcc64825c40c1d23a0b4f0",
     "grade": false,
     "grade_id": "cell-d0948d91db6d6a34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.1 (1 point WriteUp)\n",
    "Consider the sigmoid activation function for deep neural networks. Why might it lead to a \"vanishing gradient\" problem if it is used for many layers (consider plotting Q1.3)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4278c157",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d20b744c87c16cff3860cb71abd8b5b7",
     "grade": true,
     "grade_id": "cell-d89dec7f9dd5635d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<!-- <img src='plots/sigmoid.png' width=\"400px\"/> -->\n",
    "![img1](plots/sigmoid.png)\n",
    "\n",
    "As can be seen above, towards either extremity of the sigmoid, the derivative is very close to zero. Therefore, the gradient propagated back from the output layers is very small. In case of many layers, this backpropagated gradient keeps on getting multiplied with very small values, pushing it closer to zero. So, the layers receving the gradient at the end (shallower layers) receive an almost zero learning signal and can't learn anything, leading to no convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78443e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f84086fd60884f1666a86fc21b619897",
     "grade": false,
     "grade_id": "cell-ebcafd1b185b5253",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.2 (1 point WriteUp)\n",
    "Often it is replaced with $\\tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$. What are the output ranges of both $\\tanh$ and sigmoid? Why might we prefer $\\tanh$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940324a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c69505610f75e170201ac5810030f51",
     "grade": true,
     "grade_id": "cell-6ef0bff01cadddd2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Sigmoid has an output range of (0, 1) while tanh has that of (-1, 1). Tanh might be preferred over sigmoid as it is zero-centered while sigmoid is not. This would be similar to the normalized input and so, it would produce outputs which on average are closer to zero. This helps with faster convergence. Also, the derivatives are larger leading to more learning per iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa26b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c045a69bc896bfc119effaeac4fc27fa",
     "grade": false,
     "grade_id": "cell-210ca940cc6cf12f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.3 (1 point WriteUp)\n",
    "Why does $\\tanh(x)$ have less of a vanishing gradient problem? (plotting the derivatives helps! for reference: $\\tanh'(x) = 1 - \\tanh(x)^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03414d7f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67a3435ba44fcec113d0dd7c0a0f8013",
     "grade": true,
     "grade_id": "cell-8925f054fa7aeede",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<!-- <img src='plots/tanh.png' width=\"400px\"/> -->\n",
    "![img2](plots/tanh.png)\n",
    "\n",
    "As can be seen in the plot of the derivative, tanh has higher overall derivative values. This means stronger learning signal would be backpropagated as compared to the sigmoid ones, leading to less of a vanishing gradient problem for similar networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24221ba8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c739863fc4ff1b97f9a5f3903ddd925",
     "grade": false,
     "grade_id": "cell-b22c9cf732ba7c0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.4 (1 point WriteUp)\n",
    "$\\tanh$ is a scaled and shifted version of the sigmoid. Show how $\\tanh(x)$ can be written in terms of $\\sigma(x)$. (*Hint: consider how to make it have the same range*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1acfd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c71d5c8cbe29d4cf8846bb0508e636f",
     "grade": true,
     "grade_id": "cell-51d748c739cc28d1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$\\tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$ and $\\sigma(x) = \\frac{1}{1+e^{-x}}$ \n",
    "\n",
    "\n",
    "Subtracting and adding 1 in the numerator\n",
    "\n",
    "$$\\tanh(x) = \\frac{2 - 1 - e^{-2x}}{1+e^{-2x}}$$\n",
    "$$\\tanh(x) = \\frac{2}{1+e^{-2x}} - \\left(\\frac{1 + e^{-2x}}{1+e^{-2x}}\\right)$$\n",
    "$$\\tanh(x) = \\frac{2}{1+e^{-2x}} - 1$$\n",
    "$$\\tanh(x) = 2\\sigma(2x) - 1$$\n",
    "\n",
    "Hence, can be seen that $tanh$ is a scaled and shifted variant of sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48adfc1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d784dc196042141a43dd4d3ca1e02b1f",
     "grade": false,
     "grade_id": "cell-572b479fed175027",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "## For the following questions, please find the instructions in the corresponding jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd695937",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c91869b98f9b4caae54948f1b46215e6",
     "grade": false,
     "grade_id": "cell-73a9363dbb4eeb8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2 Implement a Fully Connected Network (65 points + 10 Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f107feb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ed7a931a26950507cb91fe9f3a20e74",
     "grade": false,
     "grade_id": "cell-61d6980ffe69f0cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.1.1 (3 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da6806",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "481f81209d087fc42c585a066c0d38b9",
     "grade": true,
     "grade_id": "cell-5adbb50e2e048b6c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Initializing the weights with the same initial value leads to the nodes in the same hidden layer having the exact same value. This is because each node is calculated as $z = Wx$ and since all $W$ are same, $z$ too will have same value and this continues for all the layers. Similarly, during backprop, the gradients passed along are equal and so, each weight at each layer gets updated in the similar manner. Thus, the network doesnt learn anything useful as all weights tend to remain same and essentially try to detect the same feature. The network is unable to break out of this symmetry and thus, doesn't learn anything. The weights between the input layer and hidden layer will have repeating rows while those between hidden layer and output layer will have repeating columns. Weights between 2 hidden layers will all have the same value as the gradient at every node of the same hidden layer is the same and the activated output at every node of the same hidden layer is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d8a04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37da28bf7e7b703f770662d1b369983d",
     "grade": false,
     "grade_id": "cell-377494940d117ac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.1.3 (2 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30968fff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea48c62fb88f8e96cfcca6b4750590f6",
     "grade": true,
     "grade_id": "cell-cd2436d1298e6db3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Similar to the reason above, all weights can't have the same value. So, the best option would be to sample the weights from a distribution. If the weights are too small, then the overall output produced and itheir effect is very low and so, is not useful. If the weights are too large, then the overall output could explode and thus, lead to divergence. At the same time, each node at a layer has incoming signals coming from nodes of previous layer. So, the summation of these signals would lead to a distribution with higher variance. So, more the number of previous layer nodes, higher is the variance of the outputat the current layer node. If the weight values are not ideal, the value computed (z) at that node could end up being very large (as it is a linear combination of the weights and input). So, by ensuring that weight values are sampled from a distribution with a variance inversely proportinal to the size of the layers (therefore more input signals, lower the variance), we can ensure that the outputs generated at nodes have low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c715e54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5dab96c7da53901d6339a2edd564f38",
     "grade": false,
     "grade_id": "cell-e4b82a4d357cc644",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3 Training Models (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90a554",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c94c62abd3795b9cdd129d1a7bdd950",
     "grade": false,
     "grade_id": "cell-6489cea69bd8bdd3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.2 (3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50960da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def train_nist(lr=1e-3, max_iters=200, batch_size=8, plot_loss=True, plot_acc=True):\n",
    "\n",
    "    train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "    valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "    train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "    valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "\n",
    "    max_iters = max_iters\n",
    "    # pick a batch size, learning rate\n",
    "    batch_size = batch_size\n",
    "    learning_rate = lr\n",
    "    hidden_size = 64\n",
    "\n",
    "    batches = get_random_batches(train_x,train_y,batch_size)\n",
    "    batch_num = len(batches)\n",
    "\n",
    "    params = {}\n",
    "\n",
    "    # initialize layers (named \"layer1\" and \"output\") here\n",
    "    initialize_weights(1024, 64, params, 'layer1')\n",
    "    initialize_weights(64, 36, params, 'output')\n",
    "\n",
    "    val_acc = []\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    # with default settings, you should get loss < 150 and accuracy > 80%\n",
    "    for itr in range(max_iters):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "        for xb,yb in batches:\n",
    "\n",
    "            y_probs = compute_forward(params, xb)\n",
    "\n",
    "            # loss\n",
    "            # be sure to add loss and accuracy to epoch totals            \n",
    "            loss, acc = compute_loss_and_acc(yb, y_probs)\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "\n",
    "            # backward\n",
    "            delta1 = y_probs\n",
    "            delta1[np.arange(y_probs.shape[0]), np.argmax(yb, axis = 1)] -= 1\n",
    "            delta2 = backwards(delta1, params, 'output', linear_deriv)\n",
    "            backwards(delta2, params, 'layer1')\n",
    "\n",
    "            # apply gradient\n",
    "            params['W' + 'output'] -= (learning_rate * params['grad_W' + 'output'])\n",
    "            params['b' + 'output'] -= (learning_rate * params['grad_b' + 'output'])\n",
    "            params['W' + 'layer1'] -= (learning_rate * params['grad_W' + 'layer1'])\n",
    "            params['b' + 'layer1'] -= (learning_rate * params['grad_b' + 'layer1'])\n",
    "\n",
    "        total_acc /= len(batches)\n",
    "        val_y_probs = compute_forward(params, valid_x)\n",
    "        loss, acc = compute_loss_and_acc(valid_y, val_y_probs)\n",
    "        val_acc.append(acc)\n",
    "        val_loss.append(loss)\n",
    "        train_acc.append(total_acc)\n",
    "        train_loss.append(total_loss)\n",
    "\n",
    "        if itr % 2 == 0:\n",
    "            print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f} \\t val_acc : {:.2f}\".format(itr,total_loss,total_acc, val_acc[-1]))\n",
    "\n",
    "    # run on validation set and report accuracy! should be above 75%\n",
    "    valid_acc = val_acc[-1]\n",
    "    print('Validation accuracy: ',valid_acc)\n",
    "    \n",
    "    train_loss = [loss / train_x.shape[0] for loss in train_loss]\n",
    "    val_loss = [loss / valid_x.shape[0] for loss in val_loss]\n",
    "    \n",
    "    if plot_loss:\n",
    "        plt.plot(list(range(1, max_iters + 1)), train_loss, 'ro-', label='Training')\n",
    "        plt.plot(list(range(1, max_iters + 1)), val_loss, 'bo-', label='Validation')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    if plot_acc:\n",
    "        plt.plot(list(range(1, max_iters + 1)), train_acc, 'ro-', label='Training')\n",
    "        plt.plot(list(range(1, max_iters + 1)), val_acc, 'bo-', label='Validation')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return val_loss, val_acc, train_loss, train_acc\n",
    "\n",
    "print('Alpha = 0.001')\n",
    "_ = train_nist(lr=1e-3)\n",
    "print('Alpha = 0.01')\n",
    "_ = train_nist(lr=1e-2)\n",
    "print('Alpha = 0.0001')\n",
    "_ = train_nist(lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ff42",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e29bc06161593b30c7c37dd27ea29eba",
     "grade": true,
     "grade_id": "cell-e380768cc8511806",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<!-- Alpha             |Loss             |  Accuracy\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "0.001 |![](plots/3_2_0.001_loss.png)  |   ![](plots/3_2_0.001_acc.png)\n",
    "0.01 |<img src='plots/3_2_0.01_loss.png' width=\"400px\"/>  |   <img src='plots/3_2_0.01_acc.png' width=\"400px\"/>\n",
    "0.0001 |<img src='plots/3_2_0.0001_loss.png' width=\"400px\"/>  |   <img src='plots/3_2_0.0001_acc.png' width=\"400px\"/> -->\n",
    "\n",
    "Alpha = 0.001\n",
    "\n",
    "![](plots/3_2_0.001_loss.png)\n",
    "\n",
    "![](plots/3_2_0.001_acc.png)\n",
    "\n",
    "Alpha = 0.01\n",
    "\n",
    "![](plots/3_2_0.01_loss.png)\n",
    "\n",
    "![](plots/3_2_0.01_acc.png)\n",
    "\n",
    "Alpha = 0.0001\n",
    "\n",
    "![](plots/3_2_0.0001_loss.png)\n",
    "\n",
    "![](plots/3_2_0.0001_acc.png)\n",
    "\n",
    "$\\alpha = 0.001$ performs the best with a test accuracy of 79%. $\\alpha = 0.01$ and $\\alpha = 0.01$ have a test accuracy of 74% and 67%.\n",
    "\n",
    "In case of higher learning rates, the model ends up converging very early to some suboptimal minima by taking larger steps. With high learning rate, the network would oscillate and jump over the global minima. By lowering the learning rates, we ensure that the model takes smaller steps and thus, this oscillating effect goes away, allowing the model to converge to optimal minima. However, if the learning rate is too low, the model takes very small steps towards the optima and thus, converges very slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72841ff4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3478e43dd1a1e0d9e798ec5ca6bfe1f",
     "grade": false,
     "grade_id": "cell-9a360d4e2d2a59f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.3 (2 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "# Please put your code for Q3.3 here\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "\n",
    "max_iters = 200\n",
    "# pick a batch size, learning rate\n",
    "batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 64\n",
    "\n",
    "batches = get_random_batches(train_x,train_y,batch_size)\n",
    "batch_num = len(batches)\n",
    "\n",
    "params = {}\n",
    "\n",
    "# initialize layers (named \"layer1\" and \"output\") here\n",
    "initialize_weights(1024, 64, params, 'layer1')\n",
    "initialize_weights(64, 36, params, 'output')\n",
    "\n",
    "init_weights1 = params['Wlayer1']\n",
    "init_weights1 = init_weights1.T.reshape(hidden_size, 32, 32)\n",
    "\n",
    "print('Init weights visualizations:')\n",
    "fig, ax = plt.subplots(nrows=8, ncols=8, figsize=(15,15))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        col.imshow(init_weights1[i], cmap='gray')\n",
    "        i += 1\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "# with default settings, you should get loss < 150 and accuracy > 80%\n",
    "for itr in range(max_iters):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for xb,yb in batches:\n",
    "\n",
    "        y_probs = compute_forward(params, xb)\n",
    "\n",
    "        # loss\n",
    "        # be sure to add loss and accuracy to epoch totals            \n",
    "        loss, acc = compute_loss_and_acc(yb, y_probs)\n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "\n",
    "        # backward\n",
    "        delta1 = y_probs\n",
    "        delta1[np.arange(y_probs.shape[0]), np.argmax(yb, axis = 1)] -= 1\n",
    "        delta2 = backwards(delta1, params, 'output', linear_deriv)\n",
    "        backwards(delta2, params, 'layer1')\n",
    "\n",
    "        # apply gradient\n",
    "        params['W' + 'output'] -= (learning_rate * params['grad_W' + 'output'])\n",
    "        params['b' + 'output'] -= (learning_rate * params['grad_b' + 'output'])\n",
    "        params['W' + 'layer1'] -= (learning_rate * params['grad_W' + 'layer1'])\n",
    "        params['b' + 'layer1'] -= (learning_rate * params['grad_b' + 'layer1'])\n",
    "\n",
    "    total_acc /= len(batches)\n",
    "    loss, acc = compute_loss_and_acc(valid_y, compute_forward(params, valid_x))\n",
    "\n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f} \\t val_acc : {:.2f}\".format(itr, total_loss, total_acc, acc))\n",
    "\n",
    "# run on validation set and report accuracy! should be above 75%\n",
    "valid_acc = val_acc[-1]\n",
    "print('Validation accuracy: ',valid_acc)\n",
    "\n",
    "trained_weights1 = params['Wlayer1']\n",
    "trained_weights1 = trained_weights1.T.reshape(hidden_size, 32, 32)\n",
    "\n",
    "print('Trained weights visualizations:')\n",
    "fig, ax = plt.subplots(nrows=8, ncols=8, figsize=(15,15))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        col.imshow(trained_weights1[i], cmap='gray')\n",
    "        i += 1 \n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371db3f9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b855fc6e16d07858a3ab4b060d605316",
     "grade": true,
     "grade_id": "cell-7cf3247bb27e0005",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<!-- Initial             |  Final\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src='plots/3_3_init.png' width=\"400px\"/>  |   <img src='plots/3_3_final.png' width=\"400px\"/> -->\n",
    "\n",
    "Initial Weights:\n",
    "\n",
    "![](plots/3_3_init.png)\n",
    "\n",
    "\n",
    "Final Weights:\n",
    "\n",
    "![](plots/3_3_final.png)\n",
    "\n",
    "Initially all the weights are randomly assigned. This can be seen in the visualization as we can only see random noise in each filter. However, after training the filters, they have learned to detect low level features like edges, corners, gradient shifts etc. You can see character patterns in the filters like the curves for A or V etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8e037",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82b24e4243fdda57695057b7d4ce081d",
     "grade": false,
     "grade_id": "cell-c10943ecdbefccf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.4 (3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "\n",
    "max_iters = 200\n",
    "# pick a batch size, learning rate\n",
    "batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 64\n",
    "\n",
    "batches = get_random_batches(train_x,train_y,batch_size)\n",
    "batch_num = len(batches)\n",
    "\n",
    "params = {}\n",
    "\n",
    "# initialize layers (named \"layer1\" and \"output\") here\n",
    "initialize_weights(1024, 64, params, 'layer1')\n",
    "initialize_weights(64, 36, params, 'output')\n",
    "\n",
    "# with default settings, you should get loss < 150 and accuracy > 80%\n",
    "for itr in range(max_iters):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for xb,yb in batches:\n",
    "\n",
    "        y_probs = compute_forward(params, xb)\n",
    "\n",
    "        # loss\n",
    "        # be sure to add loss and accuracy to epoch totals            \n",
    "        loss, acc = compute_loss_and_acc(yb, y_probs)\n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "\n",
    "        # backward\n",
    "        delta1 = y_probs\n",
    "        delta1[np.arange(y_probs.shape[0]), np.argmax(yb, axis = 1)] -= 1\n",
    "        delta2 = backwards(delta1, params, 'output', linear_deriv)\n",
    "        backwards(delta2, params, 'layer1')\n",
    "\n",
    "        # apply gradient\n",
    "        params['W' + 'output'] -= (learning_rate * params['grad_W' + 'output'])\n",
    "        params['b' + 'output'] -= (learning_rate * params['grad_b' + 'output'])\n",
    "        params['W' + 'layer1'] -= (learning_rate * params['grad_W' + 'layer1'])\n",
    "        params['b' + 'layer1'] -= (learning_rate * params['grad_b' + 'layer1'])\n",
    "\n",
    "    total_acc /= len(batches)\n",
    "    loss, acc = compute_loss_and_acc(valid_y, compute_forward(params, valid_x))\n",
    "\n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f} \\t val_acc : {:.2f}\".format(itr, total_loss, total_acc, acc))\n",
    "\n",
    "# run on validation set and report accuracy! should be above 75%\n",
    "valid_acc = val_acc[-1]\n",
    "print('Validation accuracy: ',valid_acc)\n",
    "\n",
    "trained_weights1 = params['Wlayer1']\n",
    "trained_weights2 = params['Woutput']\n",
    "\n",
    "filters = []\n",
    "val_images = []\n",
    "\n",
    "val_y_probs = compute_forward(params, valid_x)\n",
    "\n",
    "for j in range(36):\n",
    "    i = np.zeros((1, 36))\n",
    "    i[0, j] = 1\n",
    "    filters.append(((i @ trained_weights2.T) @ trained_weights1.T).reshape(32, 32).T)\n",
    "    val_images.append(valid_x[int(np.argmax(val_y_probs[:, np.argmax(i)]))].reshape(32, 32).T)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=6, ncols=6, figsize=(15,15))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        col.imshow(filters[i], cmap='gray')\n",
    "        col.set_title(str(i + 1))\n",
    "        i += 1 \n",
    "        \n",
    "print('Filters:')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=6, ncols=6, figsize=(15,15))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        col.imshow(val_images[i], cmap='gray')\n",
    "        col.set_title(str(i + 1))\n",
    "        i += 1 \n",
    "        \n",
    "print('Validation Images:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bcb12",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d43991574656ba1a0eb1406923f56f2",
     "grade": true,
     "grade_id": "cell-d318551e7bb2f699",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Filters:\n",
    "\n",
    "![](plots/3_4_filters.png)\n",
    "\n",
    "Validation:\n",
    "\n",
    "![](plots/3_4_data.png)\n",
    "\n",
    "The above filters grid shows the input filter responsible for activating the correposnding output class. The corresponding validation data grid contains the image that leads to the maximum activation of its class. The filters show how the weights combine to detect a particular character style. For eg., the first filter combines weights that detect something like an A. Similarly, for others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e52d12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "388f963a9bb49f6382837fcc8ee45a96",
     "grade": false,
     "grade_id": "cell-9d5e6606a4d1e707",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.5 (4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "test_data = scipy.io.loadmat('data/nist36_test.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "test_x, test_y = test_data['test_data'], test_data['test_labels']\n",
    "\n",
    "max_iters = 200\n",
    "# pick a batch size, learning rate\n",
    "batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 64\n",
    "\n",
    "batches = get_random_batches(train_x,train_y,batch_size)\n",
    "batch_num = len(batches)\n",
    "\n",
    "params = {}\n",
    "\n",
    "# initialize layers (named \"layer1\" and \"output\") here\n",
    "initialize_weights(1024, 64, params, 'layer1')\n",
    "initialize_weights(64, 36, params, 'output')\n",
    "\n",
    "# with default settings, you should get loss < 150 and accuracy > 80%\n",
    "for itr in range(max_iters):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for xb,yb in batches:\n",
    "\n",
    "        y_probs = compute_forward(params, xb)\n",
    "\n",
    "        # loss\n",
    "        # be sure to add loss and accuracy to epoch totals            \n",
    "        loss, acc = compute_loss_and_acc(yb, y_probs)\n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "\n",
    "        # backward\n",
    "        delta1 = y_probs\n",
    "        delta1[np.arange(y_probs.shape[0]), np.argmax(yb, axis = 1)] -= 1\n",
    "        delta2 = backwards(delta1, params, 'output', linear_deriv)\n",
    "        backwards(delta2, params, 'layer1')\n",
    "\n",
    "        # apply gradient\n",
    "        params['W' + 'output'] -= (learning_rate * params['grad_W' + 'output'])\n",
    "        params['b' + 'output'] -= (learning_rate * params['grad_b' + 'output'])\n",
    "        params['W' + 'layer1'] -= (learning_rate * params['grad_W' + 'layer1'])\n",
    "        params['b' + 'layer1'] -= (learning_rate * params['grad_b' + 'layer1'])\n",
    "\n",
    "    total_acc /= len(batches)\n",
    "    loss, acc = compute_loss_and_acc(valid_y, compute_forward(params, valid_x))\n",
    "\n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f} \\t val_acc : {:.2f}\".format(itr, total_loss, total_acc, acc))\n",
    "\n",
    "# run on validation set and report accuracy! should be above 75%\n",
    "valid_acc = val_acc[-1]\n",
    "print('Validation accuracy: ',valid_acc)\n",
    "\n",
    "test_y_probs = compute_forward(params, test_x)\n",
    "test_y_probs = np.argmax(test_y_probs, axis=1)\n",
    "test_y = np.argmax(test_y, axis=1)\n",
    "\n",
    "\n",
    "confusion_matrix = np.zeros((train_y.shape[1],train_y.shape[1]))\n",
    "\n",
    "for i in range(test_y_probs.shape[0]):\n",
    "    confusion_matrix[test_y[i], test_y_probs[i]] += 1\n",
    "\n",
    "import string\n",
    "plt.imshow(confusion_matrix,interpolation='nearest')\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(36),string.ascii_uppercase[:26] + ''.join([str(_) for _ in range(10)]))\n",
    "plt.yticks(np.arange(36),string.ascii_uppercase[:26] + ''.join([str(_) for _ in range(10)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e2a03",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3f8d3fe3399fe704eacaf73425da961",
     "grade": true,
     "grade_id": "cell-699c18952c5f0ecf",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![](plots/3_5.png)\n",
    "\n",
    "While there's a high overall accuracy, there are certain classes that are cconfused with others. For eg, true 'O' are confused with the digit 0 and vice versa. Similarly, digit 5s are confused wit letter 'S'. As can be seen from the mismatched classes, these have a very similar resemblance and hence, the confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd606c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48a7c17fe06eaa41b58095991a3cc574",
     "grade": false,
     "grade_id": "cell-234eebae3fbb6b14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4 Extract Text from Images (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b1bf3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6605f95b8ece1b3f46f79b915dbe13b",
     "grade": false,
     "grade_id": "cell-564cb15873d70616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.1 (3 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed9080",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e08a4d82b841f978434652fadd620ac4",
     "grade": true,
     "grade_id": "cell-371a94429864d47b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Our algorithm makes the following assumptions\n",
    "\n",
    "1 - The background and foreground will have enough contrast such that all the letter would be detected.\n",
    "\n",
    "2 - All letters are written separately so that the connected components algorithm can detect letters.\n",
    "\n",
    "I believe the algorithm would work poorly on the following examples\n",
    "\n",
    "Missing letters :\n",
    "\n",
    "![](plots/4_1_low_cont.jpg)\n",
    "\n",
    "Incorrect bounding boxes predictions:\n",
    "\n",
    "![](plots/4_1_cursive.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8c60d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4edb60ed50aae686c4440d4da43c7203",
     "grade": false,
     "grade_id": "cell-466d4c77e800b085",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.2 (13 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "import skimage\n",
    "import skimage.measure\n",
    "import skimage.color\n",
    "import skimage.restoration\n",
    "import skimage.io\n",
    "import skimage.filters\n",
    "import skimage.morphology\n",
    "import skimage.segmentation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sort_key(item1, item2):\n",
    "    line_thres = 150\n",
    "    word_thres = 150\n",
    "\n",
    "    if abs(item1[0] - item2[0]) < line_thres:\n",
    "        return item1[1] - item2[1]\n",
    "    \n",
    "    return item1[0] - item2[0]\n",
    "    \n",
    "\n",
    "# takes a color image\n",
    "# returns a list of bounding boxes and black_and_white image\n",
    "def findLetters(image):\n",
    "    bboxes = []\n",
    "    bw = skimage.color.rgb2gray(image)\n",
    "    # insert processing in here\n",
    "    bw = skimage.restoration.denoise_tv_chambolle(bw)\n",
    "    bw = skimage.morphology.opening(bw)\n",
    "    bw = skimage.morphology.closing(bw)\n",
    "    thres = skimage.filters.threshold_isodata(bw)\n",
    "    \n",
    "    background = bw >= thres\n",
    "    foreground = bw < thres\n",
    "    bw[foreground] = 0\n",
    "    bw[background] = 1\n",
    "    \n",
    "    lab_bw = skimage.measure.label(bw, 1)\n",
    "    expanded = skimage.segmentation.expand_labels(lab_bw, distance=10)\n",
    "    regionprops = skimage.measure.regionprops(expanded)\n",
    "    bboxes = [region.bbox for region in regionprops if region.area > (2 * 1024)]\n",
    "    bboxes = sorted(bboxes, key=functools.cmp_to_key(sort_key))\n",
    "    \n",
    "    return bboxes, bw\n",
    "\n",
    "image = skimage.io.imread('images/03_haiku.jpg')\n",
    "bboxes, bw = findLetters(image)\n",
    "\n",
    "bw_marked = np.stack((bw, bw, bw), axis=-1)\n",
    "\n",
    "for i in bboxes:\n",
    "    rr, cc = skimage.draw.rectangle_perimeter((i[0], i[1]), (i[2], i[3]), shape=bw_marked.shape[:2])\n",
    "    bw_marked[rr, cc] = [0, 255, 255]\n",
    "\n",
    "\n",
    "plt.imshow(bw_marked, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36408ef9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "527abea6466e5fad42c1f2d89dbb31ac",
     "grade": true,
     "grade_id": "cell-30d1533c7f070694",
     "locked": false,
     "points": 13,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<!-- <img src='plots/4_2.png' width=\"400px\"/> -->\n",
    "![](plots/4_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6842b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "167e6bb80f063a680466df23c8ed3a27",
     "grade": false,
     "grade_id": "cell-a4ab802715e8fde6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.3 (6 points WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "\n",
    "from ipynb.fs.defs.q2 import *\n",
    "\n",
    "# do not include any more libraries here!\n",
    "# no opencv, no sklearn, etc!\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "def get_patches(img_path, size=32, transpose=True):\n",
    "    im1 = skimage.img_as_float(skimage.io.imread(os.path.join('images',img_path)))\n",
    "    bboxes, bw = findLetters(im1)\n",
    "\n",
    "    plt.imshow(bw)\n",
    "    for bbox in bboxes:\n",
    "        minr, minc, maxr, maxc = bbox\n",
    "        rect = matplotlib.patches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                fill=False, edgecolor='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.show()\n",
    "    \n",
    "    patches = []\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        patch = np.pad(bw[bbox[0]: bbox[2], bbox[1]: bbox[3]], 20, mode='edge')        \n",
    "        patch = skimage.transform.resize(patch, (size, size))\n",
    "        patch = 1 - skimage.morphology.dilation(1. - patch)\n",
    "            \n",
    "        if transpose:\n",
    "            patch = patch.T\n",
    "    \n",
    "        patches.append(patch.flatten())\n",
    "    \n",
    "    patches = np.vstack(patches)\n",
    "    \n",
    "    return patches, bboxes\n",
    "\n",
    "for img in os.listdir('images'):\n",
    "    _ = get_patches(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60411b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "876f9ca6285b996ddfdfa544123ec302",
     "grade": true,
     "grade_id": "cell-d4a44957464f69b9",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![](plots/4_3_1.png)\n",
    "\n",
    "![](plots/4_3_2.png)\n",
    "\n",
    "![](plots/4_3_3.png)\n",
    "\n",
    "![](plots/4_3_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09831f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3addd5c49e15d1c3a4db909e988d4cfa",
     "grade": false,
     "grade_id": "cell-2a94753cc27fccbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.4 (13 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6430c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights\n",
    "# run the crops through your neural network and print them out\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "params = pickle.load(open('q3_weights.pickle','rb'))\n",
    "\n",
    "def show_sentences(patches, bboxes, characters, line_thres=150, word_thres=130):\n",
    "    res = str(characters[0])\n",
    "\n",
    "    for i in range(1, len(characters)):\n",
    "        if (abs(bboxes[i][0] - bboxes[i - 1][0]) > line_thres):\n",
    "            res += '\\n'\n",
    "        elif (abs((bboxes[i][3] + bboxes[i][1]) / 2 - (bboxes[i - 1][3] + bboxes[i - 1][1]) / 2) > word_thres):\n",
    "            res += ' '\n",
    "\n",
    "        res += characters[i]\n",
    "\n",
    "    print(res)\n",
    "\n",
    "for img in os.listdir('images'):\n",
    "    patches, bboxes = get_patches(img)\n",
    "    \n",
    "    # Network\n",
    "    a1 = forward(patches, params, name='layer1')\n",
    "    y_probs = forward(a1, params, name='output', activation=softmax)\n",
    "    \n",
    "    y = np.argmax(y_probs, axis=1)\n",
    "    y[y < 26] += 65\n",
    "    y[y < 36] += (48 - 26)\n",
    "    \n",
    "    show_sentences(patches, bboxes, [chr(y[i]) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c186c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9abbef4396c44e65c1bd963d38631dbe",
     "grade": true,
     "grade_id": "cell-737f94592ee1fc0c",
     "locked": false,
     "points": 13,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "![](plots/4_4_1.png)\n",
    "\n",
    "![](plots/4_4_2.png)\n",
    "\n",
    "![](plots/4_4_3.png)\n",
    "\n",
    "![](plots/4_4_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42d8cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb22c308c16293123a8cb36258be42",
     "grade": false,
     "grade_id": "cell-e5cd95eec95a2803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q5 Image Compression with Autoencoders [Extra Credit](25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212a33d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f785be92e6500cf812623c5f04def06e",
     "grade": false,
     "grade_id": "cell-8a21593a43945ffa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q5.1.1 [Extra Credit](10 points Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from ipynb.fs.defs.q2 import *\n",
    "from collections import Counter\n",
    "\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "# we don't need labels now!\n",
    "train_x = train_data['train_data']\n",
    "valid_x = valid_data['valid_data']\n",
    "\n",
    "max_iters = 150\n",
    "# pick a batch size, initial learning rate\n",
    "batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 32\n",
    "lr_rate = 50\n",
    "\n",
    "batches = get_random_batches(train_x,np.ones((train_x.shape[0],1)),batch_size)\n",
    "batch_num = len(batches)\n",
    "\n",
    "params = Counter()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# initialize layers here\n",
    "initialize_weights(1024, hidden_size, params, 'layer1')\n",
    "initialize_weights(hidden_size, hidden_size, params, 'layer2')\n",
    "initialize_weights(hidden_size, hidden_size, params, 'layer3')\n",
    "initialize_weights(hidden_size, 1024, params, 'output')\n",
    "\n",
    "def momentum_update(w: np.ndarray, m_w: np.ndarray, grad_w: np.ndarray, alpha: float, momentum: float=0.9):\n",
    "    '''\n",
    "    Momentum update\n",
    "    \n",
    "    [input]\n",
    "    * w -- parameters\n",
    "    * m_w -- the momentum of the parameters\n",
    "    * grad_x -- parameter gradients\n",
    "    * alpha -- learning rate\n",
    "    * momentum -- the momentum factor\n",
    "    \n",
    "    [output]\n",
    "    * w -- updated parameters\n",
    "    * m_w -- updated momentum of the parameters\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    m_w = momentum * m_w - alpha * grad_w\n",
    "    w = w + m_w\n",
    "\n",
    "    return w, m_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa1141",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d02ee49d26e0c7f7ec983ff8ed361c83",
     "grade": true,
     "grade_id": "cell-a14d388df159b04c",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5960d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "026c5dbdb601cd578f75ee1cf2370d98",
     "grade": false,
     "grade_id": "cell-6966f08a72f45ffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5.2 [Extra Credit](3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "\n",
    "for itr in range(max_iters):\n",
    "    total_loss = 0\n",
    "    for xb,_ in batches:\n",
    "        \n",
    "        # https://datascience.stackexchange.com/a/19581        \n",
    "        a1 = forward(xb, params, 'layer1', relu)\n",
    "        a2 = forward(a1, params, 'layer2', relu)\n",
    "        a3 = forward(a2, params, 'layer3', relu)\n",
    "        out = forward(a3, params, 'output', sigmoid)\n",
    "        \n",
    "        # loss\n",
    "        total_loss += (np.mean(np.sum((xb - out) ** 2, axis=1)))\n",
    "        \n",
    "        delta1 = 2 * (out - xb) / xb.shape[0]\n",
    "        delta2 = backwards(delta1, params, 'output', sigmoid_deriv)\n",
    "        delta3 = backwards(delta2, params, 'layer3', relu_deriv)\n",
    "        delta4 = backwards(delta3, params, 'layer2', relu_deriv)\n",
    "        delta5 = backwards(delta4, params, 'layer1', relu_deriv)\n",
    "        \n",
    "        \n",
    "        # apply gradient with momentum\n",
    "        params['W' + 'output'], params['m_W' + 'output'] = momentum_update(params['W' + 'output'],\n",
    "                                                                           params['m_W' + 'output'],\n",
    "                                                                           params['grad_W' + 'output'],\n",
    "                                                                           learning_rate)\n",
    "        params['W' + 'layer1'], params['m_W' + 'layer1'] = momentum_update(params['W' + 'layer1'],\n",
    "                                                                           params['m_W' + 'layer1'],\n",
    "                                                                           params['grad_W' + 'layer1'],\n",
    "                                                                           learning_rate)\n",
    "        params['W' + 'layer2'], params['m_W' + 'layer2'] = momentum_update(params['W' + 'layer2'],\n",
    "                                                                           params['m_W' + 'layer2'],\n",
    "                                                                           params['grad_W' + 'layer2'],\n",
    "                                                                           learning_rate)\n",
    "        params['W' + 'layer3'], params['m_W' + 'layer3'] = momentum_update(params['W' + 'layer3'],\n",
    "                                                                           params['m_W' + 'layer3'],\n",
    "                                                                           params['grad_W' + 'layer3'],\n",
    "                                                                           learning_rate)\n",
    "        params['b' + 'output'], params['m_b' + 'output'] = momentum_update(params['b' + 'output'],\n",
    "                                                                           params['m_b' + 'output'],\n",
    "                                                                           params['grad_b' + 'output'],\n",
    "                                                                           learning_rate)\n",
    "        params['b' + 'layer1'], params['m_b' + 'layer1'] = momentum_update(params['b' + 'layer1'],\n",
    "                                                                           params['m_b' + 'layer1'],\n",
    "                                                                           params['grad_b' + 'layer1'],\n",
    "                                                                           learning_rate)\n",
    "        params['b' + 'layer2'], params['m_b' + 'layer2'] = momentum_update(params['b' + 'layer2'],\n",
    "                                                                           params['m_b' + 'layer2'],\n",
    "                                                                           params['grad_b' + 'layer2'],\n",
    "                                                                           learning_rate)\n",
    "        params['b' + 'layer3'], params['m_b' + 'layer3'] = momentum_update(params['b' + 'layer3'],\n",
    "                                                                           params['m_b' + 'layer3'],\n",
    "                                                                           params['grad_b' + 'layer3'],\n",
    "                                                                           learning_rate)\n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f}\".format(itr,total_loss))\n",
    "    if itr % lr_rate == lr_rate-1:\n",
    "        learning_rate *= 0.9\n",
    "    \n",
    "    train_loss.append(total_loss)\n",
    "\n",
    "train_loss = [loss / train_x.shape[0] for loss in train_loss]\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_loss, 'ro-', label='Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b5aa5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86cc58520dc956edf4e6ce2e6e7bbb97",
     "grade": true,
     "grade_id": "cell-a372ab8ed75b12ef",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![](plots/5_2.png)\n",
    "\n",
    "The loss is decreasing over the epochs. There were instances where it started increasing but that's where the lr scheduling kicked in and lowering the learning rate led to the loss decreasing again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f314e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a154ac361c62555f562ce6ead2afced",
     "grade": false,
     "grade_id": "cell-70958da3fad26122",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q5.3.1 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf73929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = np.random.permutation(36)[:5]\n",
    "valid_x = valid_data['valid_data']\n",
    "valid_y = np.argmax(valid_data['valid_labels'], axis=1)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    image_idx = np.argwhere(valid_y == classes[i])[:2]\n",
    "    fig, ax = plt.subplots(2, 2)\n",
    "    \n",
    "    a1 = forward(valid_x[image_idx[0]].reshape(1, -1), params, 'layer1', relu)\n",
    "    a2 = forward(a1, params, 'layer2', relu)\n",
    "    a3 = forward(a2, params, 'layer3', relu)\n",
    "    out = forward(a3, params, 'output', sigmoid)\n",
    "    \n",
    "    ax[0][0].imshow(valid_x[image_idx[0]].reshape(32, 32))\n",
    "    ax[0][0].set_title(str(classes[i]))    \n",
    "    ax[0][1].imshow(out.reshape(32, 32))\n",
    "    ax[0][1].set_title(str(classes[i]))\n",
    "    \n",
    "    a1 = forward(valid_x[image_idx[1]].reshape(1, -1), params, 'layer1', relu)\n",
    "    a2 = forward(a1, params, 'layer2', relu)\n",
    "    a3 = forward(a2, params, 'layer3', relu)\n",
    "    out = forward(a3, params, 'output', sigmoid)\n",
    "    \n",
    "    ax[1][0].imshow(valid_x[image_idx[1]].reshape(32, 32))\n",
    "    ax[1][0].set_title(str(classes[i]))\n",
    "    ax[1][1].imshow(out.reshape(32, 32))\n",
    "    ax[1][1].set_title(str(classes[i]))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a5c8a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "098d955e7a2b27bd750f607b22649f8f",
     "grade": true,
     "grade_id": "cell-7fa95d191289b087",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![](plots/5_3_1.png)\n",
    "\n",
    "![](plots/5_3_2.png)\n",
    "\n",
    "![](plots/5_3_3.png)\n",
    "\n",
    "![](plots/5_3_4.png)\n",
    "\n",
    "![](plots/5_3_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf844df0",
   "metadata": {},
   "source": [
    "The reconstructions are faintly similar to the original images. Thus, the autoencoder network has learnt to represent the image features in lower dimensions pretty decently as on expansion back to higher dimension, the images are similar. There are still some differences which could be due to the different orientations of the training data belonging to that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edfab0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d821fa3a0e3d33936ed51e80b3b53437",
     "grade": false,
     "grade_id": "cell-ef6f9cc6c4e94fa5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q5.3.2 [Extra Credit](3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9843e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "# evaluate PSNR\n",
    "\n",
    "valid_x = valid_data['valid_data']\n",
    "\n",
    "total_psnr = 0.\n",
    "\n",
    "for i in range(valid_x.shape[0]):\n",
    "    \n",
    "    a1 = forward(valid_x[i].reshape(1, -1), params, 'layer1', relu)\n",
    "    a2 = forward(a1, params, 'layer2', relu)\n",
    "    a3 = forward(a2, params, 'layer3', relu)\n",
    "    out = forward(a3, params, 'output', sigmoid)\n",
    "    \n",
    "    total_psnr += psnr(valid_x[i].reshape(1, -1), out.reshape(1, -1))\n",
    "\n",
    "print('Mean PSNR - {}'.format(total_psnr / valid_x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2611e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d51a06c464b2a3e64c918cd011f4c07",
     "grade": true,
     "grade_id": "cell-6ec226a41e5e9ec2",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "I got a PSNR value of 15.064"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde62bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a643c6ea6bd559bcd4227e73b7fc6004",
     "grade": false,
     "grade_id": "cell-7d5e7253cd0fa4de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q6 Comparing against PCA [Extra Credit](15 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0870e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3acfc281c735af9489846f6a15d52b80",
     "grade": false,
     "grade_id": "cell-b9da236a37a91e2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.1 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from ipynb.fs.defs.q2 import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "# we don't need labels now!\n",
    "train_x = train_data['train_data']\n",
    "valid_x = valid_data['valid_data']\n",
    "\n",
    "dim = 32\n",
    "\n",
    "u, sig, v = np.linalg.svd(train_x)\n",
    "\n",
    "proj = v.T[:, :32]\n",
    "\n",
    "print(proj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf78efd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ad8b69f5e20d8ca53ad73e0ecd2dac3",
     "grade": true,
     "grade_id": "cell-9454330fc70ecd95",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The projection matrix is of size (1024, 32) and has a rank of 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ab825",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3369f26cb22d40444e83b4e1249201d3",
     "grade": false,
     "grade_id": "cell-1b7ec6b69ff5f438",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.2 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [7, 26, 27, 10, 34]\n",
    "valid_x = valid_data['valid_data']\n",
    "valid_y = np.argmax(valid_data['valid_labels'], axis=1)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    image_idx = np.argwhere(valid_y == classes[i])[:2]\n",
    "    fig, ax = plt.subplots(2, 2)\n",
    "    \n",
    "    out = proj @ (proj.T @ valid_x[image_idx[0]].reshape(-1, 1))\n",
    "    \n",
    "    ax[0][0].imshow(valid_x[image_idx[0]].reshape(32, 32))\n",
    "    ax[0][0].set_title(str(classes[i]))    \n",
    "    ax[0][1].imshow(out.reshape(32, 32))\n",
    "    ax[0][1].set_title(str(classes[i]))\n",
    "    \n",
    "    out = proj @ (proj.T @ valid_x[image_idx[1]].reshape(-1, 1))\n",
    "    \n",
    "    ax[1][0].imshow(valid_x[image_idx[1]].reshape(32, 32))\n",
    "    ax[1][0].set_title(str(classes[i]))\n",
    "    ax[1][1].imshow(out.reshape(32, 32))\n",
    "    ax[1][1].set_title(str(classes[i]))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32d677",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e4f860c3b38646a09b8033ae6316174",
     "grade": true,
     "grade_id": "cell-95b567b5c8381e3b",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "![](plots/6_2_1.png)\n",
    "\n",
    "![](plots/6_2_2.png)\n",
    "\n",
    "![](plots/6_2_3.png)\n",
    "\n",
    "![](plots/6_2_4.png)\n",
    "\n",
    "![](plots/6_2_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb59a8c",
   "metadata": {},
   "source": [
    "The reconstructed image ranges from ~(-0.45) to ~1.5 unlike the original which ranged from 0 to 1. However, still, PCA has learned a good projection matrix to understand the important 32 features that can successfully capture most of the original image information. These are pretty close to the autoencoder generated images. However, these ones seem to be capturing the orientation information better as the reconstructed images dont seem to have any lingering trails or edges which the autoencoder ones have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b74f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f31d56d2e923ff65f46531901d89801",
     "grade": false,
     "grade_id": "cell-78bf5924af955143",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.3 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a4512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "# evaluate PSNR\n",
    "\n",
    "valid_x = valid_data['valid_data']\n",
    "\n",
    "total_psnr = 0.\n",
    "\n",
    "for i in range(valid_x.shape[0]):\n",
    "    \n",
    "    out = proj @ (proj.T @ valid_x[i].reshape(-1, 1))    \n",
    "    total_psnr += psnr(valid_x[i].reshape(1, -1), out.reshape(1, -1))\n",
    "\n",
    "print('Mean PSNR - {}'.format(total_psnr / valid_x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91542536",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46fcbf9e5231f598b759b7c34f4d30d2",
     "grade": true,
     "grade_id": "cell-24b71dedcffa1338",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Mean PSNR - 16.284\n",
    "\n",
    "PCA outperforms autoencoder. Even though this doesnt make sense, I feel the autoencoder wasn't trained enough. The autoencoder has way too many parameters as compared to the PCA projection matrix. So, while right now PCA outperforms autoencoder, over long time with proper training, it'd autoencoder outperform PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfbac3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4e3df385828fa4f76edf8c4b0116b92",
     "grade": false,
     "grade_id": "cell-2f4c2ac42d375426",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.4 [Extra Credit](3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from ipynb.fs.defs.q2 import *\n",
    "from collections import Counter\n",
    "\n",
    "params = Counter()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# initialize layers here\n",
    "initialize_weights(1024, 32, params, 'layer1')\n",
    "initialize_weights(32, 32, params, 'layer2')\n",
    "initialize_weights(32, 32, params, 'layer3')\n",
    "initialize_weights(32, 1024, params, 'output')\n",
    "\n",
    "\n",
    "total_params = 0\n",
    "\n",
    "for k,v in params.items():\n",
    "    if '_' in k: \n",
    "        continue\n",
    "        \n",
    "    total_params += v.size\n",
    "\n",
    "print('Autoencoder - ', total_params)\n",
    "print('PCA - ', 1024 * 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215afb7c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18577ce2a29bc7dedc1f08a41a1cf78c",
     "grade": true,
     "grade_id": "cell-27922e2c1160421c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The number of parameters - \n",
    "\n",
    "Autoencoder -  68704\n",
    "\n",
    "PCA -  32768\n",
    "\n",
    "The autoencoder has way too many parameters as compared to the PCA projection matrix. So, while right now PCA outperforms autoencoder, over long time with proper training, it'd autoencoder outperform PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a97ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "896262237ed588dd156db42e7e06c08a",
     "grade": false,
     "grade_id": "cell-8fac1ca3ff947951",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q7 PyTorch (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798781ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce78f233d97e1f4541527a039a629fa1",
     "grade": false,
     "grade_id": "cell-03dd8625c09f576b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.1 (10 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 100\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "\n",
    "tensor_trainx = torch.Tensor(train_x)\n",
    "tensor_trainy = torch.Tensor(train_y)\n",
    "tensor_valx = torch.Tensor(valid_x)\n",
    "tensor_valy = torch.Tensor(valid_y)\n",
    "\n",
    "train_dataset = tdata.TensorDataset(tensor_trainx, tensor_trainy)\n",
    "val_dataset = tdata.TensorDataset(tensor_valx, tensor_valy)\n",
    "\n",
    "trainloader = tdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = tdata.DataLoader(val_dataset, batch_size=valid_x.shape[0])\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1024, 64)\n",
    "        self.fc2 = nn.Linear(64, 36)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(F.sigmoid(self.fc1(x)))\n",
    "#         x = self.fc2(self.sigmoid(self.fc1(x)))\n",
    "#         x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optim = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "\n",
    "for itr in range(max_iters):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    avg_acc = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        x, y = data\n",
    "        labels = torch.argmax(y, axis=1)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        avg_acc += (torch.sum(labels == torch.argmax(outputs, axis=1)) / len(labels))\n",
    "    \n",
    "    train_loss.append(running_loss)\n",
    "    train_acc.append(avg_acc / len(trainloader))\n",
    "    \n",
    "    x, y = next(iter(valloader))\n",
    "    labels = torch.argmax(y, axis=1)\n",
    "    outputs = net(x)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    val_loss.append(loss.item())\n",
    "    val_acc.append(torch.sum(labels == torch.argmax(outputs, axis=1)) / len(labels))\n",
    "    \n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f} \\t val_acc : {:.2f}\".format(\n",
    "            itr, train_loss[-1], train_acc[-1], val_acc[-1]))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "train_loss = [loss / train_x.shape[0] for loss in train_loss]\n",
    "val_loss = [loss / valid_x.shape[0] for loss in val_loss]\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_loss, 'ro-', label='Training')\n",
    "plt.plot(list(range(1, max_iters + 1)), val_loss, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_acc, 'ro-', label='Training')\n",
    "plt.plot(list(range(1, max_iters + 1)), val_acc, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2e507",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e86a489666ac68fabc4f8d8bf152b612",
     "grade": true,
     "grade_id": "cell-5ffc6cb938d61d80",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![](plots/7_1_1_loss.png)\n",
    "![](plots/7_1_1_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b6567",
   "metadata": {},
   "source": [
    "Final validation accuracy - 77.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c88cf1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69eafedd527ad9c3a8f669acddb315e4",
     "grade": false,
     "grade_id": "cell-b310f2ddba0f6647",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.2 (3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5332c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"device = {}\".format(device))\n",
    "\n",
    "print(\"Get dataset\")\n",
    "mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "trainset_loader = DataLoader(mnist_train, batch_size=20, shuffle=True, num_workers=1)\n",
    "\n",
    "mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "testset_loader = DataLoader(mnist_test, batch_size=len(mnist_train), shuffle=False, num_workers=1)\n",
    "\n",
    "print(\"dataset size train, test\")\n",
    "print(trainset_loader.dataset.data.shape)\n",
    "print(testset_loader.dataset.data.shape)\n",
    "\n",
    "max_iters = 30\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1 - Input -> Conv(5x5x1x6) -> Pool(f=2, s=2)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        # layer 2 - Input -> Conv(5x5x6x16) -> Pool(f=2, s=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        # layer 3 - Input -> Conv(5x5x16x32) -> Pool(f=2, s=2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5, padding=2)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # layer4 - (288) -> 120\n",
    "        self.fc4 = nn.Linear(288, 120)\n",
    "        \n",
    "        # layer5 - (120) -> 84\n",
    "        self.fc5 = nn.Linear(120, 84)\n",
    "        \n",
    "        # layer6 - 84 - 10\n",
    "        self.fc6 = nn.Linear(84, 10)   \n",
    "\n",
    "    def forward(self, x):        \n",
    "        #one forward pass\n",
    "        z1 = self.conv1(x)\n",
    "        a1 = self.pool1(F.relu(z1))\n",
    "        \n",
    "        z2 = self.conv2(a1)\n",
    "        a2 = self.pool2(F.relu(z2))\n",
    "        \n",
    "        z3 = self.conv3(a2)\n",
    "        a3 = self.pool3(F.relu(z3))\n",
    "        \n",
    "        z4 = self.fc4(self.flatten(a3))\n",
    "        a4 = F.relu(z4)\n",
    "        \n",
    "        z5 = self.fc5(a4)\n",
    "        a5 = F.relu(z5)\n",
    "        \n",
    "        out = self.fc6(a5)\n",
    "        \n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optim = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "for itr in range(max_iters):\n",
    "    running_loss = 0.0\n",
    "    avg_acc = 0.0\n",
    "    for i, data in enumerate(trainset_loader):\n",
    "        x, y = data\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        avg_acc += (torch.sum(y == torch.argmax(outputs, axis=1)) / len(y))\n",
    "\n",
    "    train_loss.append(running_loss)\n",
    "    train_acc.append(avg_acc / len(trainset_loader))\n",
    "    \n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(\n",
    "            itr, train_loss[-1], train_acc[-1]))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "train_loss = [loss / train_x.shape[0] for loss in train_loss]\n",
    "val_loss = [loss / valid_x.shape[0] for loss in val_loss]\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_loss, 'ro-', label='Training')\n",
    "# plt.plot(list(range(1, max_iters + 1)), val_loss, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_acc, 'ro-', label='Training')\n",
    "# plt.plot(list(range(1, max_iters + 1)), val_acc, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "data, labels = next(iter(testset_loader))\n",
    "test_outputs = net(data)\n",
    "acc = torch.sum(labels == torch.argmax(test_outputs, dim=1)) / data.shape[0]\n",
    "\n",
    "print('Test Accuracy - {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e539c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cacd397b428d4eadd73ea230f56d108",
     "grade": true,
     "grade_id": "cell-f5a9affcedad4b2e",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![](plots/7_1_2_loss.png)\n",
    "![](plots/7_1_2_acc.png)\n",
    "\n",
    "Test Accuracy - 99.13%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac5571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10e4fe6fb3561465bf1d2be58eac4cd8",
     "grade": false,
     "grade_id": "cell-36d48a100a6ddb8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.3 (2 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"device = {}\".format(device))\n",
    "\n",
    "print(\"Get dataset\")\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'].astype(np.float32), train_data['train_labels'].astype(np.int)\n",
    "valid_x, valid_y = valid_data['valid_data'].astype(np.float32), valid_data['valid_labels'].astype(np.int)\n",
    "\n",
    "max_iters = 30\n",
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "\n",
    "tensor_trainx = torch.Tensor(train_x.reshape(-1, 1, 32, 32))\n",
    "tensor_trainy = torch.Tensor(train_y)\n",
    "tensor_valx = torch.Tensor(valid_x.reshape(-1, 1, 32, 32))\n",
    "tensor_valy = torch.Tensor(valid_y)\n",
    "\n",
    "train_dataset = tdata.TensorDataset(tensor_trainx, tensor_trainy)\n",
    "val_dataset = tdata.TensorDataset(tensor_valx, tensor_valy)\n",
    "\n",
    "trainloader = tdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = tdata.DataLoader(val_dataset, batch_size=valid_x.shape[0])\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1 - Input -> Conv(5x5x1x6) -> Pool(f=2, s=2)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        # layer 2 - Input -> Conv(5x5x6x16) -> Pool(f=2, s=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        # layer 3 - Input -> Conv(5x5x16x32) -> Pool(f=2, s=2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5, padding=2)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # layer4 - (512) -> 120\n",
    "        self.fc4 = nn.Linear(512, 120)\n",
    "        \n",
    "        # layer5 - (120) -> 84\n",
    "        self.fc5 = nn.Linear(120, 84)\n",
    "        \n",
    "        # layer6 - 84 - 10\n",
    "        self.fc6 = nn.Linear(84, 36)   \n",
    "\n",
    "    def forward(self, x):        \n",
    "        #one forward pass\n",
    "        z1 = self.conv1(x)\n",
    "        a1 = self.pool1(F.relu(z1))\n",
    "        \n",
    "        z2 = self.conv2(a1)\n",
    "        a2 = self.pool2(F.relu(z2))\n",
    "        \n",
    "        z3 = self.conv3(a2)\n",
    "        a3 = self.pool3(F.relu(z3))\n",
    "        \n",
    "        z4 = self.fc4(self.flatten(a3))\n",
    "        a4 = F.relu(z4)\n",
    "        \n",
    "        z5 = self.fc5(a4)\n",
    "        a5 = F.relu(z5)\n",
    "        \n",
    "        out = self.fc6(a5)\n",
    "        \n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optim = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "for itr in range(max_iters):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    avg_acc = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        x, y = data\n",
    "        labels = torch.argmax(y, axis=1)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "\n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        avg_acc += (torch.sum(labels == torch.argmax(outputs, axis=1)) / len(y))\n",
    "    \n",
    "    train_loss.append(running_loss)\n",
    "    train_acc.append(avg_acc / len(trainloader))\n",
    "    \n",
    "    x, y = next(iter(valloader))\n",
    "    labels = torch.argmax(y, axis=1)\n",
    "    outputs = net(x)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    val_loss.append(loss.item())\n",
    "    val_acc.append(torch.sum(labels == torch.argmax(outputs, axis=1)) / len(labels))\n",
    "    \n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f} \\t val_acc : {:.2f}\".format(\n",
    "            itr, train_loss[-1], train_acc[-1], val_acc[-1]))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "train_loss = [loss / train_x.shape[0] for loss in train_loss]\n",
    "val_loss = [loss / valid_x.shape[0] for loss in val_loss]\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_loss, 'ro-', label='Training')\n",
    "plt.plot(list(range(1, max_iters + 1)), val_loss, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_acc, 'ro-', label='Training')\n",
    "plt.plot(list(range(1, max_iters + 1)), val_acc, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ca9ee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74715a525b6ea868aa46e9612811a809",
     "grade": true,
     "grade_id": "cell-1e34acb3833e499c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "![](plots/7_1_3_loss.png)\n",
    "![](plots/7_1_3_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0f26a",
   "metadata": {},
   "source": [
    "Final validation accuracy - 88.73%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c82e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dbd040f3376e7d6a32cdcb0395dcce8",
     "grade": false,
     "grade_id": "cell-646081b13d109d59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.4 (15 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897998fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import EMNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"device = {}\".format(device))\n",
    "\n",
    "print(\"Get dataset\")\n",
    "\n",
    "EMNIST.url = 'http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip'\n",
    "# Reference for transform function\n",
    "# https://stackoverflow.com/a/54513835\n",
    "transform=torchvision.transforms.Compose([\n",
    "    lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
    "    lambda img: torchvision.transforms.functional.hflip(img),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "emnist_train = EMNIST(root=\"data\", split='balanced', train=True, download=True, transform=transform)\n",
    "\n",
    "trainset_loader = DataLoader(emnist_train, batch_size=20, shuffle=True, num_workers=1)\n",
    "\n",
    "emnist_test = EMNIST(root=\"data\", split='balanced', train=False, download=True, transform=transform)\n",
    "testset_loader = DataLoader(emnist_test, batch_size=20, shuffle=True, num_workers=1)\n",
    "\n",
    "# Ref: https://github.com/gaurav0651/emnist/blob/master/train_emnist.ipynb\n",
    "label_map = ['0','1','2','3','4','5','6','7','8','9',\n",
    "       'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\n",
    "       'a','b','d','e','f','g','h','n','q','r','t']\n",
    "\n",
    "print(trainset_loader.dataset.data.shape)\n",
    "print(testset_loader.dataset.data.shape)\n",
    "\n",
    "max_iters = 50\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1 - Input -> Conv(5x5x1x6) -> Pool(f=2, s=2)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        # layer 2 - Input -> Conv(5x5x6x16) -> Pool(f=2, s=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        # layer 3 - Input -> Conv(5x5x16x32) -> Pool(f=2, s=2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5, padding=2)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # layer4 - (288) -> 120\n",
    "        self.fc4 = nn.Linear(288, 120)\n",
    "        \n",
    "        # layer5 - (120) -> 84\n",
    "        self.fc5 = nn.Linear(120, 84)\n",
    "        \n",
    "        # layer6 - 84 - 47\n",
    "        self.fc6 = nn.Linear(84, 47)   \n",
    "\n",
    "    def forward(self, x):        \n",
    "        #one forward pass\n",
    "        z1 = self.conv1(x)\n",
    "        a1 = self.pool1(F.relu(z1))\n",
    "        \n",
    "        z2 = self.conv2(a1)\n",
    "        a2 = self.pool2(F.relu(z2))\n",
    "        \n",
    "        z3 = self.conv3(a2)\n",
    "        a3 = self.pool3(F.relu(z3))\n",
    "        \n",
    "        z4 = self.fc4(self.flatten(a3))\n",
    "        a4 = F.relu(z4)\n",
    "        \n",
    "        z5 = self.fc5(a4)\n",
    "        a5 = F.relu(z5)\n",
    "        \n",
    "        out = self.fc6(a5)\n",
    "        \n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optim = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "for itr in range(max_iters):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    avg_acc = 0.0\n",
    "    for i, data in enumerate(trainset_loader):\n",
    "        x, y = data\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        avg_acc += (torch.sum(y == torch.argmax(outputs, axis=1)) / len(y))\n",
    "    \n",
    "    train_loss.append(running_loss)\n",
    "    train_acc.append(avg_acc / len(trainset_loader))\n",
    "    \n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(\n",
    "            itr, train_loss[-1], train_acc[-1]))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "train_loss = [loss / len(trainset_loader) for loss in train_loss]\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_loss, 'ro-', label='Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_acc, 'ro-', label='Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "data, labels = next(iter(testset_loader))\n",
    "test_outputs = net(data)\n",
    "acc = torch.sum(labels == torch.argmax(test_outputs, dim=1)) / data.shape[0]\n",
    "\n",
    "print('Test Accuracy - {0}'.format(acc))\n",
    "\n",
    "net.eval()\n",
    "\n",
    "for img in os.listdir('images'):\n",
    "    patches, bboxes = get_patches(img, 28, transpose=False)\n",
    "    \n",
    "    patches = 1 - patches\n",
    "    \n",
    "    # Network    \n",
    "    x = torch.from_numpy(patches.reshape(len(patches), 1, 28, 28)).to(device).float()\n",
    "    y_probs = net(x).detach().numpy()\n",
    "    y = np.argmax(y_probs, axis=1)\n",
    "    characters = [label_map[y[i]] for i in range(y.shape[0])]    \n",
    "    show_sentences(patches, bboxes, characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9275d1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7224a041d0e860f947177e01f87d6729",
     "grade": true,
     "grade_id": "cell-506e3bc2f687b05d",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<!-- Loss             |  Accuracy\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src='plots/7_1_4_loss.png' width=\"400px\"/>  |   <img src='plots/7_1_4_acc.png' width=\"400px\"/> -->\n",
    "![](plots/7_1_4_loss.png)\n",
    "![](plots/7_1_4_acc.png)\n",
    "\n",
    "Test Accuracy - 80%\n",
    "\n",
    "![](plots/7_1_4_1.png)\n",
    "![](plots/7_1_4_2.png)\n",
    "![](plots/7_1_4_3.png)\n",
    "![](plots/7_1_4_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74cf45f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4abc20e42d98374cefff61d298a72b58",
     "grade": false,
     "grade_id": "cell-08d1cfcc2156f6e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.2.1 (10 points WriteUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Code for fine-tune squeezenet1_1\n",
    "max_iters = 20\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_classes = 17\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.RandomSizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "train_dataset = torchvision.datasets.ImageFolder('data/flowers17/train', transform=train_transform)\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "val_dataset = torchvision.datasets.ImageFolder('data/flowers17/val', transform=val_transform)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "test_dataset = torchvision.datasets.ImageFolder('data/flowers17/test', transform=test_transform)\n",
    "\n",
    "print(f'{len(val_dataset) = }')\n",
    "print(f'{len(test_dataset) = }')\n",
    "\n",
    "trainloader = tdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = tdata.DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "testloader = tdata.DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "net = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)\n",
    "net.classifier[1] = nn.Conv2d(512, num_classes, 1)\n",
    "net.num_classes = num_classes\n",
    "\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in net.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "for itr in range(max_iters):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    avg_acc = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        x, y = data\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        avg_acc += (torch.sum(y == torch.argmax(outputs, axis=1)) / len(y))\n",
    "    \n",
    "    train_loss.append(running_loss)\n",
    "    train_acc.append(avg_acc / len(trainloader))\n",
    "    \n",
    "    x, y = next(iter(valloader))\n",
    "    outputs = net(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    val_loss.append(loss.item())\n",
    "    val_acc.append(torch.sum(y == torch.argmax(outputs, axis=1)) / len(y))\n",
    "    \n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f} \\t val_loss : {:.2f} \\t val_acc : {:.2f}\".format(\n",
    "            itr, train_loss[-1], train_acc[-1], val_loss[-1], val_acc[-1]))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "train_loss = [loss / len(train_dataset) for loss in train_loss]\n",
    "val_loss = [loss / len(val_dataset) for loss in val_loss]\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_loss, 'ro-', label='Training')\n",
    "plt.plot(list(range(1, max_iters + 1)), val_loss, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_acc, 'ro-', label='Training')\n",
    "plt.plot(list(range(1, max_iters + 1)), val_acc, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "data, labels = next(iter(testloader))\n",
    "test_outputs = net(data)\n",
    "acc = torch.sum(labels == torch.argmax(test_outputs, dim=1)) / data.shape[0]\n",
    "\n",
    "print('Test Accuracy - {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397af2d5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16fb1e938c1dc10d26a79e0b2d19dcc5",
     "grade": true,
     "grade_id": "cell-f8bcdfeff7c2b335",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "![](plots/7_2_1_loss.png)\n",
    "![](plots/7_2_1_acc.png)\n",
    "\n",
    "Test Accuracy - 0.9117646813392639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f89242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "max_iters = 100\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_classes = 17\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.RandomSizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "train_dataset = torchvision.datasets.ImageFolder('data/flowers17/train', transform=train_transform)\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "val_dataset = torchvision.datasets.ImageFolder('data/flowers17/val', transform=val_transform)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "test_dataset = torchvision.datasets.ImageFolder('data/flowers17/test', transform=test_transform)\n",
    "\n",
    "print(f'{len(val_dataset) = }')\n",
    "print(f'{len(test_dataset) = }')\n",
    "\n",
    "trainloader = tdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = tdata.DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "testloader = tdata.DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1 - Input -> Conv(5x5x1x6) -> Pool(f=2, s=2)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        # layer 2 - Input -> Conv(5x5x6x16) -> Pool(f=2, s=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        # layer 3 - Input -> Conv(5x5x16x32) -> Pool(f=2, s=2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5, padding=2)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # layer4 - (25088) -> 256\n",
    "        self.fc4 = nn.Linear(25088, 256)\n",
    "        \n",
    "        # layer5 - (256) -> 84\n",
    "        self.fc5 = nn.Linear(256, 84)\n",
    "        \n",
    "        # layer6 - 84 - 17\n",
    "        self.fc6 = nn.Linear(84, 17)   \n",
    "\n",
    "    def forward(self, x):        \n",
    "        #one forward pass\n",
    "        z1 = self.conv1(x)\n",
    "        a1 = self.pool1(F.relu(z1))\n",
    "        \n",
    "        z2 = self.conv2(a1)\n",
    "        a2 = self.pool2(F.relu(z2))\n",
    "        \n",
    "        z3 = self.conv3(a2)\n",
    "        a3 = self.pool3(F.relu(z3))\n",
    "        \n",
    "        z4 = self.fc4(self.flatten(a3))\n",
    "        a4 = F.relu(z4)\n",
    "        \n",
    "        z5 = self.fc5(a4)\n",
    "        a5 = F.relu(z5)\n",
    "        \n",
    "        out = self.fc6(a5)\n",
    "        \n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "for itr in range(max_iters):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    avg_acc = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        x, y = data\n",
    "        \n",
    "        optim.zero_grad()        \n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        avg_acc += (torch.sum(y == torch.argmax(outputs, axis=1)) / len(y))\n",
    "    \n",
    "    train_loss.append(running_loss)\n",
    "    train_acc.append(avg_acc / len(trainloader))\n",
    "    \n",
    "    x, y = next(iter(valloader))\n",
    "    outputs = net(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    val_loss.append(loss.item())\n",
    "    val_acc.append(torch.sum(y == torch.argmax(outputs, axis=1)) / len(y))\n",
    "    \n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f} \\t val_loss : {:.2f} \\t val_acc : {:.2f}\".format(\n",
    "            itr, train_loss[-1], train_acc[-1], val_loss[-1], val_acc[-1]))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "train_loss = [loss / len(train_dataset) for loss in train_loss]\n",
    "val_loss = [loss / len(val_dataset) for loss in val_loss]\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_loss, 'ro-', label='Training')\n",
    "plt.plot(list(range(1, max_iters + 1)), val_loss, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(1, max_iters + 1)), train_acc, 'ro-', label='Training')\n",
    "plt.plot(list(range(1, max_iters + 1)), val_acc, 'bo-', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "data, labels = next(iter(testloader))\n",
    "test_outputs = net(data)\n",
    "acc = torch.sum(labels == torch.argmax(test_outputs, dim=1)) / data.shape[0]\n",
    "\n",
    "print('Test Accuracy - {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c0d0c",
   "metadata": {},
   "source": [
    "![](plots/7_2_2_loss.png)  \n",
    "![](plots/7_2_2_acc.png)\n",
    "\n",
    "Test Accuracy - 0.7147058844566345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e6828",
   "metadata": {},
   "source": [
    "The finetuned squeezenet (test acc - 91.2%) works better than the custom network (test acc - 71.5%) trained from scratch. The squeezenet has a bigger/deeper architecture and has weights that have been pretrained on imagenets whereas the custom network is way shallower than squeeznet and has been trained from scratch and only for a 100 epochs. So, the difference in performance is not at all surprising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d51a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01537177398acd3fc733f5ecfc3a6f9d",
     "grade": false,
     "grade_id": "cell-d502ae43d1374766",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Appendix: Neural Network Overview\n",
    "Deep learning has quickly become one of the most applied machine learning techniques in computer vision. Convolutional neural networks have been applied to many different computer vision problems such as image classification, recognition, and segmentation with great success. In this assignment, you will first implement a fully connected feed forward neural network for hand written character classification. Then in the second part, you will implement a system to locate characters in an image, which you can then classify with your deep network. The end result will be a system that, given an image of hand written text, will output the text contained in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b71b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e751aa5f73b563407f3df529cdc3eba1",
     "grade": false,
     "grade_id": "cell-4e449e493f11f2c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Basic Use\n",
    "Here we will give a brief overview of the math for a single hidden layer feed forward network. For a more detailed look at the math and derivation, please see the class slides.\n",
    "\n",
    "A fully-connected network $\\textbf{f}$, for classification, applies a series of linear and non-linear functions to an input data vector $\\textbf{x}$ of size $N\\times 1$ to produce an output vector $\\textbf{f}(\\textbf{x})$ of size $C\\times 1$, where each element $i$ of the output vector represents the probability of $\\textbf{x}$ belonging to the class $i$. Since the data samples are of dimensionality $N$, this means the input layer has $N$ input units. To compute the value of the output units, we must first compute the values of all the hidden layers. The first hidden layer *pre-activation* $\\textbf{a}^{(1)}(\\textbf{x})$ is given by\n",
    "\n",
    "$$\\textbf{a}^{(1)}(\\textbf{x}) = \\textbf{W}^{(1)}\\textbf{x} + \\textbf{b}^{(1)}$$\n",
    "\n",
    "Then the *post-activation* values of the first hidden layer $\\textbf{h}^{(1)}(\\textbf{x})$ are computed by applying a non-linear activation function $\\textbf{g}$ to the *pre-activation* values\n",
    "\n",
    "$$\\textbf{h}^{(1)}(\\textbf{x}) = \\textbf{g}(\\textbf{a}^{(1)}(\\textbf{x})) = \\textbf{g}(\\textbf{W}^{(1)}\\textbf{x} + \\textbf{b}^{(1)})$$\n",
    "\n",
    "Subsequent hidden layer ($1 < t \\leq T$) pre- and post activations are given by:\n",
    "\n",
    "$$\\textbf{a}^{(t)}(\\textbf{x}) = \\textbf{W}^{(t)}\\textbf{h}^{(t-1)} + \\textbf{b}^{(t)}$$\n",
    "\n",
    "$$\\textbf{h}^{(t)}(\\textbf{x}) = \\textbf{g}(\\textbf{a}^{(t)}(\\textbf{x}))$$\n",
    "\n",
    "The output layer *pre-activations* $\\textbf{a}^{(T)}(\\textbf{x})$ are computed in a similar way\n",
    "\n",
    "$$\\textbf{a}^{(T)}(\\textbf{x}) = \\textbf{W}^{(T)}\\textbf{h}^{(T-1)}(\\textbf{x}) + \\textbf{b}^{(T)}$$\n",
    "\n",
    "and finally the \\emph{post-activation} values of the output layer are computed with\n",
    "$$\\textbf{f}(\\textbf{x}) = \\textbf{o}(\\textbf{a}^{(T)}(\\textbf{x})) = \\textbf{o}(\\textbf{W}^{(T)}\\textbf{h}^{(T-1)}(\\textbf{x}) + \\textbf{b}^{(T)})$$\n",
    "\n",
    "where $\\textbf{o}$ is the output activation function. Please note the difference between $\\textbf{g}$ and $\\textbf{o}$! \n",
    "For this assignment, we will be using the sigmoid activation function for the hidden layer, so:\n",
    "$$\\textbf{g}(y) = \\frac{1}{1+\\exp(-y)}$$\n",
    "where when $\\textbf{g}$ is applied to a vector, it is applied element wise across the vector.\n",
    "\n",
    "Since we are using this deep network for classification, a common output activation function to use is the softmax function. This will allow us to turn the real value, possibly negative values of $\\textbf{a}^{(T)}(\\textbf{x})$ into a set of probabilities (vector of positive numbers that sum to 1). Letting $\\textbf{x}_i$ denote the $i^{th}$ element of the vector $\\textbf{x}$, the softmax function is defined as:\n",
    "$$\\textbf{o}_i(\\textbf{y}) = \\frac{\\exp(\\textbf{y}_i)}{\\sum_j \\exp(\\textbf{y}_j)}$$\n",
    "\n",
    "![](figures/letter_montage.jpg)\n",
    "<center>Samples from NIST Special 19  dataset</center>\n",
    "\n",
    "\n",
    "Gradient descent is an iterative optimisation algorithm, used to find the local optima. To find the local minima, we start at a point on the function and move in the direction of negative gradient (steepest descent) till some stopping criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a57dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "917c876a76a8dd2304785000bd206f6a",
     "grade": false,
     "grade_id": "cell-c6f5b4cd57160fca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Backprop\n",
    "The update equation for a general weight $W^{(t)}_{ij}$ and bias $b^{(t)}_i$ is\n",
    "$$\n",
    "W^{(t)}_{ij} = W^{(t)}_{ij} - \\alpha*\\frac{\\partial L_{\\textbf{f}}}{\\partial W^{(t)}_{ij}}(\\textbf{x})\\hspace{1cm}\n",
    "b^{(t)}_{i} = b^{(t)}_{i} - \\alpha*\\frac{\\partial L_{\\textbf{f}}}{\\partial b^{(t)}_{i}}(\\textbf{x})\n",
    "$$\n",
    "$\\alpha$ is the learning rate. Please refer to the back-propagation slides for more details on how to derive the gradients. Note that here we are using softmax loss (which is different from the least square loss in the slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fecfbd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec744557890bf229ea3a4f21a9b3b4e8",
     "grade": false,
     "grade_id": "cell-67800a69b8b95457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1]  Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. 2010. http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf.\n",
    "\n",
    "[2]  P. J. Grother. Nist special database 19 – handprinted forms and characters database. https://www.nist.gov/srd/nist-special-database-19, 1995."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
