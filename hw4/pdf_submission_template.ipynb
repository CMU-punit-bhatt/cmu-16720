{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expired-modification",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90d6d6a6e781d4183b528544a2045ee4",
     "grade": false,
     "grade_id": "q1-note2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1: Homography Theory\n",
    "\n",
    "Suppose we have two cameras $\\mathbf{C}_1$ and $\\mathbf{C}_2$ looking at a common plane $\\Pi$ in 3D space.  Any 3D point $\\mathbf{P}$ on $\\Pi$ generates a\n",
    "projected 2D point located at $\\mathbf{p} \\equiv (u_1,v_1,1)^T$ on the first\n",
    "camera $\\mathbf{C}_1$ and $\\mathbf{q} \\equiv (u_2,v_2,1)^T$ on the second camera\n",
    "$\\mathbf{C}_2$.  Since $\\mathbf{P}$ is confined to the plane $\\Pi$, we expect\n",
    "that there is a relationship between $\\mathbf{p}$ and $\\mathbf{q}$.  In\n",
    "particular, there exists a common 3 $\\times$ 3 matrix $\\mathbf{H}$, so that for\n",
    "any $\\mathbf{p}$ and $\\mathbf{q}$, the following conditions holds:\n",
    "\\begin{equation}\n",
    "\\mathbf{p \\equiv Hq}\n",
    "\\label{eq:homography}\n",
    "\\end{equation}\n",
    "We call this relationship **planar homography**.  Recall that both\n",
    "$\\mathbf{p}$ and $\\mathbf{q}$ are in homogeneous coordinates and the equality\n",
    "$\\equiv$ means $\\mathbf{p}$ is proportional to $\\mathbf{Hq}$ (recall homogeneous\n",
    "coordinates).  It turns out this\n",
    "relationship is also true for cameras that are related by pure rotation without the planar constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-trail",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84867b1201d44d40d53b2f2bca681d99",
     "grade": false,
     "grade_id": "q1-note3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Homography (5 points)\n",
    "\n",
    "Prove that there exists an $\\mathbf{H}$ that\n",
    "satisfies $\\mathbf{p \\equiv Hq}$ given two ${3 \\times 4}$ camera projection\n",
    "matrices $\\mathbf{M_1}$ and $\\mathbf{M_2}$ corresponding to cameras\n",
    "$\\mathbf{C_1}$, $\\mathbf{C_2}$ and a plane $\\Pi$.  Do not produce an actual\n",
    "algebraic expression for $\\mathbf{H}$.  All we are asking for is a proof of\n",
    "the existence of $\\mathbf{H}$.\n",
    "\n",
    "*Note: A degenerate case may happen when the\n",
    "plane $\\Pi$ contains both cameras' centers, in which case there are infinite\n",
    "choices of $\\mathbf{H}$ satisfying $\\mathbf{p \\equiv Hq}$.  You can\n",
    "ignore this case in your answer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-mainstream",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the projection of 3D homogeneous point $\\mathbf{P}$ on plane $\\mathbf{\\Pi}$ to the two camera planes as $\\mathbf{p}$ and $\\mathbf{q}$\n",
    "\n",
    "$$\\therefore \\mathbf{p \\equiv M_1P}$$\n",
    "$$\\therefore \\mathbf{q \\equiv M_2P}$$\n",
    "\n",
    "\n",
    "Since $\\mathbf{p}$ and $\\mathbf{q}$ are related to the same planar point $\\mathbf{P}$ by a system of linear equations, there exists a system of linear equations that relates $\\mathbf{p}$ and $\\mathbf{q}$ as linear transformations are closed under composition.\n",
    "\n",
    "Therefore, there exists an $\\mathbf{H}$ that satisfies $\\mathbf{p \\equiv Hq}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-tanzania",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e57a9900a6e42308160ffe52f855eac",
     "grade": false,
     "grade_id": "q1-note4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Homography under rotation (5 points)\n",
    "\n",
    "Prove that there exists a homography $\\mathbf{H}$ that satisfies $\\mathbf{p_1} \\equiv \\mathbf{Hp_2}$, given two cameras separated by a pure rotation. That is, for camera 1, $\\mathbf{p_1} = \\mathbf{K_1} \\begin{bmatrix} \\mathbf{I} & \\mathbf{0} \\end{bmatrix} \\mathbf{P}$ and for camera 2, $\\mathbf{p_2} = \\mathbf{K_2} \\begin{bmatrix}\\mathbf{R} & \\mathbf{0} \\end{bmatrix} \\mathbf{P}$. Note that $\\mathbf{K_1}$ and $\\mathbf{K_2}$ are the $3 \\times 3$ intrinsic matrices of the two cameras and are different. $\\mathbf{I}$ is $3 \\times 3$ identity matrix, $\\mathbf{0}$ is a $3\\times1$ zero vector and $\\mathbf{P}$ is the homogeneous coordinate of a point in 3D space. $\\mathbf{R}$ is the $3 \\times 3$ rotation matrix of the camera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-movement",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Writing homogeneous coordinate, $\\mathbf{P} = \\begin{bmatrix} \\mathbf{X} \\\\ 1\\end{bmatrix}$ where $\\mathbf{X}$ are the 3D world coordinates.\n",
    "\n",
    "$$\\therefore \\mathbf{p_1} \\equiv \\mathbf{K_1} \\begin{bmatrix} \\mathbf{I} & \\mathbf{0} \\end{bmatrix}\\begin{bmatrix} \\mathbf{X} \\\\ 1\\end{bmatrix} \\equiv \\mathbf{K_1}\\mathbf{X}$$\n",
    "\n",
    "$$\\therefore \\mathbf{p_2} \\equiv \\mathbf{K_2} \\begin{bmatrix} \\mathbf{R} & \\mathbf{0} \\end{bmatrix}\\begin{bmatrix} \\mathbf{X} \\\\ 1\\end{bmatrix} \\equiv \\mathbf{K_2}\\mathbf{R}\\mathbf{X}$$\n",
    "\n",
    "From the second equation, $$\\mathbf{X} \\equiv (\\mathbf{K_2}\\mathbf{R})^{-1}\\mathbf{p_2}$$\n",
    "\n",
    "Substituting this in the first equation,\n",
    "\n",
    "$$\\therefore \\mathbf{p_1} \\equiv \\mathbf{K_1}\\mathbf{X} = \\mathbf{K_1}(\\mathbf{K_2}\\mathbf{R})^{-1}\\mathbf{p_2}$$\n",
    "$$\\therefore \\mathbf{p_1} \\equiv \\mathbf{K_1}\\mathbf{R}^{-1}\\mathbf{K_2}^{-1}\\mathbf{p_2}$$\n",
    "\n",
    "where $\\mathbf{K_1}\\mathbf{R}^{-1}\\mathbf{K_2}^{-1}$ is a 3x3 matrix.\n",
    "\n",
    "Hence, can be seen that there exists a 3x3 homography ($\\mathbf{H} = \\mathbf{K_1}\\mathbf{R}^{-1}\\mathbf{K_2}^{-1}$ ) from $\\mathbf{p_2}$ to $\\mathbf{p_1}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-lease",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed7c9db6948ddb8877b01a20f4925db2",
     "grade": false,
     "grade_id": "q1-note5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Correspondences (10 points)\n",
    "\n",
    "Let $\\mathbf{x_1}$ be a set of points in an image and $\\mathbf{x_2}$ be the set of corresponding points in an image taken by another camera. Suppose there exists a homography $\\mathbf{H}$ such that:\n",
    "$\\mathbf{x^i_1} \\equiv \\mathbf{Hx^i_2}\\quad(i \\in \\{1\\dots N\\})$\n",
    "where $\\mathbf{x^i_1} = \\left[\\begin{matrix}x^i_1 & y^i_1 & 1\\end{matrix}\\right]^T$ are in homogenous coordinates, $\\mathbf{x_1^i \\in \\mathbf{x_1}}$ and $\\mathbf{H}$ is a $3 \\times 3$ matrix. For each point pair, this relation can be rewritten as\n",
    "\\begin{equation*}\n",
    "\\mathbf{A_ih} = 0\n",
    "\\end{equation*}\n",
    "where $\\mathbf{h}$ is a column vector reshaped from $\\mathbf{H}$, and $\\mathbf{A_i}$ is a matrix with elements derived from the points $\\mathbf{x^i_1}$ and $\\mathbf{x^i_2}$. This can help calculate **H** from the given point correspondences.\n",
    "\n",
    "   - How many degrees of freedom does $\\mathbf{h}$ have? (3 points)\n",
    "\n",
    "   - How many point pairs are required to solve $\\mathbf{h}$? (2 points)\n",
    "\n",
    "   - Derive $\\mathbf{A_i}$. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-geneva",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**h** has 8 degrees of freedom. So, the number of point pairs required to solve for **h** is 4.\n",
    "\n",
    "Given, $\\mathbf{x^i_1} \\equiv \\mathbf{Hx^i_2}$\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "x^i_1 \\\\\n",
    "y^i_1 \\\\\n",
    "1\n",
    "\\end{bmatrix} \\equiv \\begin{bmatrix}\n",
    "h11 & h12 & h13 \\\\\n",
    "h21 & h22 & h23 \\\\\n",
    "h31 & h32 & h33 \n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "x^i_2 \\\\\n",
    "y^i_2 \\\\\n",
    "1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Simplifying RHS\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "h11 & h12 & h13 \\\\\n",
    "h21 & h22 & h23 \\\\\n",
    "h31 & h32 & h33 \n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "x^i_2 \\\\\n",
    "y^i_2 \\\\\n",
    "1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "h11 * x^i_2 + h12 * y^i_2 + h13\\\\\n",
    "h21 * x^i_2 + h22 * y^i_2 + h23\\\\\n",
    "h31 * x^i_2 + h32 * y^i_2 + h33\\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "In order for this to match this to $\\mathbf{x^i_1}$, need to make the last element 1\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "x^i_1 \\\\\n",
    "y^i_1 \\\\\n",
    "1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{h11 * x^i_2 + h12 * y^i_2 + h13}{h31 * x^i_2 + h32 * y^i_2 + h33}\\\\\n",
    "\\frac{h21 * x^i_2 + h22 * y^i_2 + h23}{h31 * x^i_2 + h32 * y^i_2 + h33}\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\therefore x^i_1 = \\frac{h11 * x^i_2 + h12 * y^i_2 + h13}{h31 * x^i_2 + h32 * y^i_2 + h33}$$\n",
    "$$\\therefore y^i_1 = \\frac{h21 * x^i_2 + h22 * y^i_2 + h23}{h31 * x^i_2 + h32 * y^i_2 + h33}$$\n",
    "\n",
    "\n",
    "$$\\therefore (h31 * x^i_2 + h32 * y^i_2 + h33)x^i_1 = h11 * x^i_2 + h12 * y^i_2 + h13$$\n",
    "$$\\therefore (h31 * x^i_2 + h32 * y^i_2 + h33)y^i_1 = h21 * x^i_2 + h22 * y^i_2 + h23$$\n",
    "\n",
    "$$\\therefore - h11 * x^i_2 - h12 * y^i_2 - h13 + x^i_1x^i_2 * h31 + x^i_1y^i_2 * h32 + x^i_1 * h33 = 0 $$\n",
    "$$\\therefore - h21 * x^i_2 - h22 * y^i_2 - h23 + y^i_1x^i_2 * h31 + y^i_1y^i_2 * h32 + y^i_1 * h33 = 0 $$\n",
    "\n",
    "Writing this in matrix form,\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "- x^i_2 & - y^i_2 & -1 & 0 & 0 & 0 x^i_1x^i_2 & x^i_1y^i_2 & x^i_1\\\\\n",
    "0 & 0 & 0 & - x^i_2 & - y^i_2 & -1 & y^i_1x^i_2 & y^i_1y^i_2 & y^i_1\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h11\\\\\n",
    "h12\\\\\n",
    "h13\\\\\n",
    "h21\\\\\n",
    "h22\\\\\n",
    "h23\\\\\n",
    "h31\\\\\n",
    "h32\\\\\n",
    "h33\n",
    "\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$$\n",
    "\n",
    "Hence, the matrix, $$A_i = \\begin{bmatrix}\n",
    "- x^i_2 & - y^i_2 & -1 & 0 & 0 & 0 x^i_1x^i_2 & x^i_1y^i_2 & x^i_1\\\\\n",
    "0 & 0 & 0 & - x^i_2 & - y^i_2 & -1 & y^i_1x^i_2 & y^i_1y^i_2 & y^i_1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-bathroom",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21c1f9a3ed29227280c319a060f9a655",
     "grade": false,
     "grade_id": "q1-note6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.4 Understanding homographies under rotation (5 points)\n",
    "\n",
    "Suppose that a camera is rotating about its center $\\mathbf{C}$, keeping the intrinsic parameters $\\mathbf{K}$ constant. Let $\\mathbf{H}$ be the homography that maps the view from one camera orientation to the view at a second orientation. Let $\\theta$ be the angle of rotation between the two. Show that $\\mathbf{H^2}$ is the homography corresponding to a rotation of 2$\\theta$. Please limit your answer within a couple of lines. A lengthy proof indicates that you're doing something too complicated (or wrong)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-affect",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Considering that the camera's only being rotated around its center, $\\mathbf{H}$ would be of the form\n",
    "\n",
    "$$ \\mathbf{H} =\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) & 0\\\\\n",
    "\\sin(\\theta) & \\cos(\\theta) & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Computing, $\\mathbf{H}^2$\n",
    "\n",
    "$$ \\mathbf{H}^2 =\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) & 0\\\\\n",
    "\\sin(\\theta) & \\cos(\\theta) & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) & 0\\\\\n",
    "\\sin(\\theta) & \\cos(\\theta) & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\cos^2(\\theta) - \\sin^2(\\theta) & -(2\\sin(\\theta)\\cos(\\theta)) & 0\\\\\n",
    "2\\sin(\\theta)\\cos(\\theta)) & \\cos^2(\\theta) - \\sin^2(\\theta) & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\cos(2\\theta) & -\\sin(2\\theta) & 0\\\\\n",
    "\\sin(2\\theta) & \\cos(2\\theta) & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Therefore, $\\mathbf{H}^2$ is the homography corresponding to a rotation of $2\\theta$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-success",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc12026419304f939c2764b0ef68890f",
     "grade": false,
     "grade_id": "q1-note7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 Limitations of the planar homography (2 points)\n",
    "\n",
    "Why is the planar homography not completely sufficient to map any arbitrary scene image to another viewpoint? State your answer concisely in one or two sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03534cba-4d31-48c4-81e0-a86aa3c74ff2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This expects scene points to lie on a plane which might not always be possible when there's a significant depth involved. Besides that, a translation could lead to an occluded viewpoint from which the object of interest/scene is no longer completely visible.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-submission",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "719b48cf270e17e6ebadf444a0df3065",
     "grade": false,
     "grade_id": "q1-note8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.6 Behavior of lines under perspective projections (3 points)\n",
    "\n",
    "We stated in class that perspective projection preserves lines (a line in 3D is projected to a line in 2D). Verify algebraically that this is the case, i.e., verify that the projection $\\mathbf{P}$ in $\\mathbf{x} = \\mathbf{PX}$ preserves lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c158f3d-651b-4fbe-8d73-3ffa5470d08f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the 3D line to be given by $(x_0, y_0, z_0) + t(a, b, c)$ where $(a, b, c)$ represent the direction vector, $(x_0, y_0, z_0)$ represents one point lying on the line and $t \\in \\mathbb{R}$ is the stepping variable that helps satisfy all points lying on this line.\n",
    "\n",
    "$$\\therefore \\mathbf{X} = \\begin{bmatrix} t*a + x_0 \\\\t*b + y_0 \\\\ t*c + z_0 \\end{bmatrix}$$\n",
    "\n",
    "Also, the projection matrix can be broken down into intrinsic and extrinsic parameter matrices. Assuming that there's no rotation or translation,\n",
    "\n",
    "$$\\mathbf{x} \\equiv \n",
    "\\alpha \\begin{bmatrix} f_x & 0 & o_x \\\\ 0 & f_y & o_y \\\\ 0 & 0 & 1\\end{bmatrix}\n",
    "\\begin{bmatrix} t*a + x_0 \\\\t*b + y_0 \\\\ t*c + z_0 \\end{bmatrix}$$\n",
    "\n",
    "where $\\alpha$ is the scaling factor\n",
    "\n",
    "$$\\mathbf{x} \\equiv \n",
    "\\alpha \\begin{bmatrix} f_x * (t*a + x_0) + o_x * (t*c + z_0) \\\\f_y * (t*b + y_0) + o_y * (t*c + z_0) \\\\ t*c + z_0 \\end{bmatrix}$$\n",
    "\n",
    "Making these homogeneous\n",
    "\n",
    "$$\\mathbf{x} =\n",
    "\\alpha \\begin{bmatrix} \\frac{f_x * (t*a + x_0) + o_x * (t*c + z_0)}{t*c + z_0} \\\\\\frac{f_y * (t*b + y_0) + o_y * (t*c + z_0)}{t*c + z_0} \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "$$\\therefore x = \\alpha * f_x * \\frac{t*a + x_0}{t*c + z_0} + \\alpha * o_x$$\n",
    "$$\\therefore y = \\alpha * f_y * \\frac{t*b + y_0}{t*c + z_0} + \\alpha * o_y$$\n",
    "\n",
    "\n",
    "Now, consider 2 3-D points lying on the 3-D line - $(x_0, y_0, z_0) + t_1(a, b, c)$ and $(x_0, y_0, z_0) + t_2(a, b, c)$  \n",
    "\n",
    "The corresponding 2D coordinates would be given by\n",
    "\n",
    "$$\\therefore x_1 = \\alpha * f_x * \\frac{t_1*a + x_0}{t_1*c + z_0} + \\alpha * o_x$$\n",
    "$$\\therefore y_1 = \\alpha * f_y * \\frac{t_1*b + y_0}{t_1*c + z_0} + \\alpha * o_y$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\therefore x_2 = \\alpha * f_x * \\frac{t_2*a + x_0}{t_2*c + z_0} + \\alpha * o_x$$\n",
    "$$\\therefore y_2 = \\alpha * f_y * \\frac{t_2*b + y_0}{t_2*c + z_0} + \\alpha * o_y$$\n",
    "\n",
    "Finding the slope of the line between these 2 points\n",
    "\n",
    "$$m = \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{\\left[\\alpha * f_y * \\frac{t_2*b + y_0}{t_2*c + z_0} + \\alpha * o_y\\right] - \\left[\\alpha * f_y * \\frac{t_1*b + y_0}{t_1*c + z_0} + \\alpha * o_y\\right]}{\\left[\\alpha * f_x * \\frac{t_2*a + x_0}{t_2*c + z_0} + \\alpha * o_x \\right]- \\left[\\alpha * f_x * \\frac{t_1*a + x_0}{t_1*c + z_0} + \\alpha * o_x\\right]} $$\n",
    "\n",
    "Simplifying,\n",
    "\n",
    "$$m = \\frac{\\left[f_y * (t_2*b + y_0)(t_1*c + z_0)\\right] - \\left[f_y * (t_1*b + y_0)(t_2*c + z_0) \\right]}{\\left[f_x * (t_2*a + x_0)(t_1*c + z_0)\\right]- \\left[f_x * (t_1*a + x_0)(t_2*c + z_0)\\right]}\n",
    "= \\frac{f_y}{f_x}\\left[\\frac{t_1 t_2 b c + t_1 c y_0 + t_2 b z_0 + y_0 z_0 - t_1 t_2 b c - t_2 c y_0 - t_1 b z_0 - y_0 z_0}{t_1 t_2 a c + t_1 c x_0 + t_2 a z_0 + x_0 z_0 - t_1 t_2 a c - t_2 c x_0 - t_1 a z_0 - x_0 z_0}\\right]\n",
    "= \\frac{f_y}{f_x}\\left[\\frac{t_1 c y_0 + t_2 b z_0 - t_2 c y_0 - t_1 b z_0}{t_1 c x_0 + t_2 a z_0 - t_2 c x_0 - t_1 a z_0}\\right]$$\n",
    "\n",
    "$$m = \\frac{f_y}{f_x}\\left[\\frac{c y_0 (t_1 - t_2) - b z_0 (t_1 - t_2)}{c x_0 (t_1 - t_2) - a z_0 (t_1 - t_2)}\\right] = \\frac{f_y}{f_x} \\frac{(t1 - t2)}{(t1 - t2)} \\left[\\frac{c y_0 - b z_0 }{c x_0 - a z_0 }\\right]$$\n",
    "\n",
    "$$\\therefore m = \\frac{f_y}{f_x} \\left[\\frac{c y_0 - b z_0 }{c x_0 - a z_0 }\\right]$$\n",
    "\n",
    "As can be seen, the slope, $m$, is independent of the variable t. Therefore, for any point lying on the 3D line, it would be projected to this 2D line with slope $m$ using the projection matrix, $\\mathbf{P}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-costs",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6304540627a2f009463acebb867ebe03",
     "grade": false,
     "grade_id": "q2_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 2.4 Check Point: Descriptor Matching (5 pts)\n",
    "\n",
    "Save the resulting figure and submit it in your PDF. Briefly discuss any cases that perform worse or better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d168d01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f76003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBrief(gaussPyramid, locsDoG, compareX, compareY, patch_width=9):\n",
    "    \n",
    "    left_limit = patch_width // 2\n",
    "    right_limit_h = gaussPyramid.shape[0] - patch_width // 2\n",
    "    right_limit_w = gaussPyramid.shape[1] - patch_width // 2\n",
    "    \n",
    "    valid_locsDoG = locsDoG[np.logical_and(np.logical_and(locsDoG[:, 0] > left_limit,\n",
    "                                                          locsDoG[:, 0] < right_limit_h),\n",
    "                                           np.logical_and(locsDoG[:, 1] > left_limit,                                                           \n",
    "                                                          locsDoG[:, 1] < right_limit_w))]\n",
    "    \n",
    "    desc = []\n",
    "    \n",
    "    for i in range(valid_locsDoG.shape[0]):\n",
    "        patch = gaussPyramid[valid_locsDoG[i, 0] - patch_width // 2: valid_locsDoG[i, 0] + patch_width // 2 + 1,\n",
    "                             valid_locsDoG[i, 1] - patch_width // 2: valid_locsDoG[i, 1] + patch_width // 2 + 1,\n",
    "                             valid_locsDoG[i, 2]]\n",
    "        patch = patch.flatten()\n",
    "        tau = patch[compareX] < patch[compareY]\n",
    "        desc.append(list(map(lambda j: str(int(tau[j])), range(len(tau)))))        \n",
    "            \n",
    "    newlocs = valid_locsDoG[:, -2: -4: -1]\n",
    "    desc = np.stack(desc)\n",
    "    \n",
    "    return newlocs, desc\n",
    "\n",
    "def briefLite(im):    \n",
    "    locsDoG, gauss_pyramid = DoGdetector(im)\n",
    "    [compareX, compareY] = np.load('data/testPattern.npy')\n",
    "    \n",
    "    locs, desc = computeBrief(gauss_pyramid, locsDoG, compareX, compareY)\n",
    "    \n",
    "    return locs, desc\n",
    "\n",
    "def briefMatch(desc1, desc2, ratio=0.8):\n",
    "    D = cdist(np.float32(desc1), np.float32(desc2), metric='hamming')\n",
    "    # find the smallest distance\n",
    "    ix2 = np.argmin(D, axis=1)\n",
    "    d1 = D.min(1)\n",
    "    # find the second smallest distance\n",
    "    d12 = np.partition(D, 2, axis=1)[:,0:2]\n",
    "    d2 = d12.max(1)\n",
    "    r = d1/(d2+1e-10)\n",
    "    is_discr = r<ratio\n",
    "    ix2 = ix2[is_discr]\n",
    "    ix1 = np.arange(D.shape[0])[is_discr]\n",
    "    matches = np.stack((ix1,ix2), axis=-1)\n",
    "    return matches\n",
    "\n",
    "def plotMatches(im1, im2, matches, locs1, locs2):\n",
    "    fig = plt.figure()\n",
    "    # draw two images side by side\n",
    "    imH = max(im1.shape[0], im2.shape[0])\n",
    "    im = np.zeros((imH, im1.shape[1]+im2.shape[1]), dtype='uint8')\n",
    "    im[0:im1.shape[0], 0:im1.shape[1]] = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im[0:im2.shape[0], im1.shape[1]:] = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    for i in range(matches.shape[0]):\n",
    "        pt1 = locs1[matches[i,0], 0:2]\n",
    "        pt2 = locs2[matches[i,1], 0:2].copy()\n",
    "        pt2[0] += im1.shape[1]\n",
    "        x = np.asarray([pt1[0], pt2[0]])\n",
    "        y = np.asarray([pt1[1], pt2[1]])\n",
    "        plt.plot(x,y,'r')\n",
    "        plt.plot(x,y,'g.')\n",
    "    plt.show()\n",
    "\n",
    "img1 = cv2.imread('data/model_chickenbroth.jpg')\n",
    "locs1, desc1 = briefLite(img1)\n",
    "\n",
    "img2 = cv2.imread('data/chickenbroth_01.jpg')\n",
    "locs2, desc2 = briefLite(img2)\n",
    "\n",
    "matches = briefMatch(desc1, desc2)\n",
    "plotMatches(img1, img2, matches, locs1, locs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9da587-ff97-4736-b29d-e1052712c153",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src='results/q2_4.png'/>\n",
    "\n",
    "BRIEF descriptors are not invariant to scale and rotational changes. So, in case of same image being rotated or upscaled/downscaled, the descriptors generated between these two images wouldn't match.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-gabriel",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "874a6f5e970a402493faad319340249e",
     "grade": false,
     "grade_id": "q2_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 2.5 BRIEF and rotations (5 pts)\n",
    "\n",
    "Include your code and the historgram figure in your PDF, and explain why you think  the descriptor behaves this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e566e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('data/model_chickenbroth.jpg')\n",
    "locs1, desc1 = briefLite(img1)\n",
    "\n",
    "img2 = cv2.imread('data/model_chickenbroth.jpg')   \n",
    "h, w, _ = img2.shape\n",
    "\n",
    "counts = []\n",
    "\n",
    "for i in range(0, 370, 10):\n",
    "    \n",
    "    # could've just used this matrix again and again. However, the img2 quality is getting worse\n",
    "    # so, matches are reducing.\n",
    "    M = cv2.getRotationMatrix2D((w / 2, h / 2), i, 1.0)\n",
    "    rotated = cv2.warpAffine(img2, M, (w, h))    \n",
    "    locs2, desc2 = briefLite(rotated)\n",
    "    matches = briefMatch(desc1, desc2)\n",
    "    \n",
    "    counts.append(len(matches))\n",
    "    \n",
    "plt.plot(list(range(0, 370, 10)), counts, 'ro-')\n",
    "plt.xlabel('Angle of Rotation')\n",
    "plt.ylabel('#Matches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0851c9-b30c-4eb0-a3ab-bdf8fa25472a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<img src='results/q2_5.png'/>\n",
    "\n",
    "The descriptor works by looking at n pairs of pixel in a patch around a keypoint. However, because of rotation, the pixel locations have locally changed within the patch. Thus, the n pairs are no longer referring to the same pixels as the ones in the unrotated image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-conditioning",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04d3f0c8590b34301c10fae68138cea1",
     "grade": false,
     "grade_id": "q2_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.6 Improving Performance - (Extra Credit, 10 pts)\n",
    "\n",
    "The extra credit opportunities described below are optional and provide an\n",
    "avenue to explore computer vision and improve the performance of the techniques developed above.\n",
    "\n",
    "   1. ($\\textbf{5 pts}$) As we have seen, BRIEF is not rotation invariant. Design a simple fix to solve this problem using the tools you have developed so far (think back to edge detection and/or Harris corner's covariance matrix).  Include yout code in your PDF, and explain your design decisions and how you selected any parameters that you use. Demonstrate the effectiveness of your algorithm on image pairs related by large rotation.\n",
    "\n",
    "   2. ($\\textbf{5 pts}$) This implementation of BRIEF has some scale invariance, but there are limits.  What happens when you match a picture to the same picture at half the size?  Look to section 3 of [Lowe2004](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf) for a technique that will make your detector more robust to changes in scale. Implement it and demonstrate it in action with several test images. Include yout code and the test images in your PDF. You may simply rescale some of the test images we have given you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRBrief(gaussPyramid, locsDoG, compareX, compareY, patch_width=9):\n",
    "    \n",
    "    h, w, _ = gaussPyramid.shape\n",
    "    left_limit = patch_width // 2\n",
    "    right_limit_h = gaussPyramid.shape[0] - patch_width // 2\n",
    "    right_limit_w = gaussPyramid.shape[1] - patch_width // 2\n",
    "    \n",
    "    valid_locsDoG = locsDoG[np.logical_and(np.logical_and(locsDoG[:, 0] > left_limit,\n",
    "                                                          locsDoG[:, 0] < right_limit_h),\n",
    "                                           np.logical_and(locsDoG[:, 1] > left_limit,                                                           \n",
    "                                                          locsDoG[:, 1] < right_limit_w))]\n",
    "    \n",
    "    desc = []\n",
    "    \n",
    "    for i in range(valid_locsDoG.shape[0]):\n",
    "        patch = gaussPyramid[valid_locsDoG[i, 0] - patch_width // 2: valid_locsDoG[i, 0] + patch_width // 2 + 1,\n",
    "                             valid_locsDoG[i, 1] - patch_width // 2: valid_locsDoG[i, 1] + patch_width // 2 + 1,\n",
    "                             valid_locsDoG[i, 2]]\n",
    "        \n",
    "#         patch = patch.flatten()\n",
    "        m10 = np.sum(np.repeat(np.arange(valid_locsDoG[i, 0] - patch_width // 2,\n",
    "                                         valid_locsDoG[i, 0] + patch_width // 2 + 1).reshape(1, -1),\n",
    "                               patch_width, axis=0) * patch)\n",
    "        m01 = np.sum(np.repeat(np.arange(valid_locsDoG[i, 1] - patch_width // 2,\n",
    "                                         valid_locsDoG[i, 1] + patch_width // 2 + 1).reshape(1, -1),\n",
    "                               patch_width, axis=0) * patch)\n",
    "        m00 = np.sum(patch)\n",
    "        theta = - np.arctan2(m01, m10)\n",
    "        R = np.array([[np.cos(theta), - np.sin(theta)],\n",
    "                      [np.sin(theta), np.cos(theta)]])\n",
    "        \n",
    "#         print(R.shape)\n",
    "#         print(np.stack((compareX.flatten(), compareY.flatten())).shape)\n",
    "        \n",
    "        compareMat = np.int32(R @ np.stack((compareX.flatten(), compareY.flatten())))\n",
    "        \n",
    "        tau = gaussPyramid[(int(valid_locsDoG[i, 0] - patch_width // 2) + compareMat[0].flatten() // patch_width).clip(0, h - 1),\n",
    "                           (int(valid_locsDoG[i, 1] - patch_width // 2) + compareMat[0].flatten() % patch_width).clip(0, w - 1),\n",
    "                           valid_locsDoG[i, 2]] < \\\n",
    "              gaussPyramid[(int(valid_locsDoG[i, 0] - patch_width // 2) + compareMat[1].flatten() // patch_width).clip(0, h - 1),\n",
    "                           (int(valid_locsDoG[i, 1] - patch_width // 2) + compareMat[1].flatten() % patch_width).clip(0, w - 1),\n",
    "                           valid_locsDoG[i, 2]]\n",
    "        desc.append(list(map(lambda j: str(int(tau[j])), range(len(tau)))))        \n",
    "            \n",
    "    newlocs = valid_locsDoG[:, -2: -4: -1]\n",
    "    desc = np.stack(desc)\n",
    "    \n",
    "    return newlocs, desc\n",
    "\n",
    "def rBriefLite(im):\n",
    "    '''\n",
    "    Given an image, detect the keypoints and describe the keypoints with descriptors.\n",
    "    \n",
    "    INPUTS\n",
    "        im - gray image with values between 0 and 1\n",
    "    OUTPUTS\n",
    "        locs - an m x 3 vector, where the first two columns are the image coordinates \n",
    "                of keypoints and the third column is the pyramid level of the keypoints\n",
    "        desc - an m x n bits matrix of stacked BRIEF descriptors. \n",
    "                m is the number of valid descriptors in the image and will vary\n",
    "                n is the number of bits for the BRIEF descriptor\n",
    "    '''\n",
    "    # Hint: Use the provided \"DoGdetector()\" obtain the smoothed Gaussian pyramid and\n",
    "    # the keypoints, and use them as the input of \"computeBrief()\"\n",
    "    \n",
    "    locsDoG, gauss_pyramid = DoGdetector(im)\n",
    "    [compareX, compareY] = np.load('data/testPattern.npy')\n",
    "    \n",
    "    locs, desc = computeRBrief(gauss_pyramid, locsDoG, compareX, compareY)\n",
    "    \n",
    "    return locs, desc\n",
    "\n",
    "img1 = cv2.imread('data/model_chickenbroth.jpg')\n",
    "# img1 = cv2.imread('data/chickenbroth_01.jpg')\n",
    "locs1, desc1 = rBriefLite(img1)\n",
    "\n",
    "img2 = cv2.imread('data/chickenbroth_01.jpg')\n",
    "\n",
    "h, w, _ = img2.shape\n",
    "\n",
    "M = cv2.getRotationMatrix2D((w / 2, h / 2), 20, 1.0)\n",
    "rotated = cv2.warpAffine(img2, M, (w, h)) \n",
    "locs2, desc2 = rBriefLite(rotated)  \n",
    "\n",
    "matches = briefMatch(desc1, desc2)\n",
    "plotMatches(img1, rotated, matches, locs1, locs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf40736-6e19-446c-b0c6-b48587eb126a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "Result:\n",
    "\n",
    "<img src='results/q2_6.png'>\n",
    "\n",
    "The idea to solve for rotation invariance was something similar to what ORB descriptor does. Took the patch and found its rotation and found the new points to compare by rotating with the same angle in the opposite direction. This would help resolve the patch rotation issue.\n",
    "\n",
    "A fix for scale invariance would be to use a guassian pyramid. By having the gaussian pyramid of an image and comparing between combinations of the other image and different pyramid levels would lead to a scale invariant system.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-factory",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4232656e2e4c13c850b080dbbfabe031",
     "grade": false,
     "grade_id": "q3_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.3 Automated Homography Estimation/Warping for Augmented Reality (10 points)\n",
    "\n",
    "Implement the following steps: \n",
    "   1. Reads $\\texttt{cv_cover.jpg}$, $\\texttt{cv_desk.png}$, and $\\texttt{hp_cover.jpg}$.\n",
    "   2. Computes a homography automatically using $\\texttt{computeH_ransac}$.\n",
    "   3. Warps $\\texttt{hp_cover.jpg}$ to the dimensions of the $\\texttt{cv_desk.png}$ image using the OpenCV $\\texttt{warpPerspective}$ function. \n",
    "   4. At this point you should notice that although the image is being warped to the correct location, it is not filling up the same space as the book. Why do you think this is happening? How would you modify $\\texttt{hp_cover.jpg}$ to fix this issue?\n",
    "   5. Implement the function: $\\texttt{function [ composite_img ] = compositeH( H2to1, template, img) }$ to now compose this warped image with the desk image as in the following figures.\n",
    "   6. Include your resulting image in your write-up. Please also print the final H matrix in your writeup (normalized so the bottom right value is 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc194c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compositeH(H, template, img):\n",
    "        \n",
    "    warped_img = cv2.warpPerspective(template, np.linalg.inv(H), img.shape[-2: -4: -1])\n",
    "    composite_img = np.uint8(warped_img == 0) * img + warped_img\n",
    "    \n",
    "    return composite_img.astype(np.uint8)\n",
    "\n",
    "im1 = cv2.cvtColor(cv2.imread(\"figure/cv_cover.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "im2 = cv2.cvtColor(cv2.imread(\"figure/cv_desk.png\"), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "locs1, desc1 = briefLite(im1)\n",
    "locs2, desc2 = briefLite(im2)\n",
    "matches = briefMatch(desc1, desc2)\n",
    "\n",
    "H, inliers = computeH_ransac(matches, locs1, locs2)\n",
    "\n",
    "print('H: {0}'.format(H / H[2, 2]))\n",
    "\n",
    "template = cv2.cvtColor(cv2.imread(\"figure/hp_cover.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resizing image to meet the im1 shape so that homography matrix works properly.\n",
    "resized_template = cv2.resize(template, im1.shape[-2: -4: -1])\n",
    "\n",
    "composite_img = compositeH(H, resized_template, im2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 15))\n",
    "\n",
    "axes[0].imshow(im2)\n",
    "axes[0].set_title('Text Book')\n",
    "axes[1].imshow(resized_template)\n",
    "axes[1].set_title('Resized HP')\n",
    "axes[2].imshow(composite_img)\n",
    "axes[2].set_title('Composite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c955486-39b6-416c-98ba-80f13c0fc540",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "$$H= \\begin{bmatrix}\n",
    "2.35252906e+00 & 6.88051228e-01 & -6.86719269e+02\\\\\n",
    "1.01883048e-01 & 4.25832288e+00 & -8.42107269e+02\\\\\n",
    "1.96661190e-04 & 3.70549187e-03 & 1.00000000e+00\\\\\n",
    " \\end{bmatrix}$$\n",
    "\n",
    "<img src='results/q3_3.png'/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-hours",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c3f862f1bc038844cc25fde833787be",
     "grade": false,
     "grade_id": "q4_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.1 Image Stitching (5 pts)\n",
    "\n",
    "Visualize the warped image. Please include the image and your H2to1 matrix (with the bottom right index as 1) in your writeup PDF, along with stating which image pair you used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d76656",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efe319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageStitching(im1, im2, H2to1, pad=((0, 0), (0, 500), (0, 0))):\n",
    "    '''\n",
    "    Returns a panorama of im1 and im2 using the given \n",
    "    homography matrix\n",
    "\n",
    "    INPUT\n",
    "        Warps img2 into img1 reference frame using the provided warpH() function\n",
    "        H2to1 - a 3 x 3 matrix encoding the homography that best matches the linear\n",
    "                 equation.\n",
    "    OUTPUT\n",
    "        img_pano - the panorama image.\n",
    "    '''\n",
    "    \n",
    "    padded_im1 = np.pad(im1, pad)    \n",
    "    warped_img = cv2.warpPerspective(im2, H2to1, padded_im1.shape[-2: -4: -1])\n",
    "    \n",
    "    # Generating weights for alpha blending.\n",
    "    w1 = distance_transform_edt(np.pad(np.ones_like(im1[1: -1, 1: -1, 0]), 1))\n",
    "    w2 = distance_transform_edt(np.pad(np.ones_like(im2[1: -1, 1: -1, 0]), 1))\n",
    "    \n",
    "    # Padding w1 as required to maintain right weight distribution.\n",
    "    padded_w1 = np.pad(w1, pad[0: 2]) \n",
    "    padded_w1 = padded_w1.reshape(padded_w1.shape[0], -1, 1)\n",
    "    \n",
    "    # Warping weights for im2 to the target shape.\n",
    "    warped_w2 = cv2.warpPerspective(w2, H2to1, padded_im1.shape[-2: -4: -1])\n",
    "    warped_w2 = warped_w2.reshape(warped_w2.shape[0], -1, 1)\n",
    "    \n",
    "    img_pano = np.uint8((padded_im1 * padded_w1 + warped_img * warped_w2) / (padded_w1 + warped_w2))\n",
    "    \n",
    "    return img_pano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac2ecc-e648-4cf5-aa47-767432056ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$H2to1= \\begin{bmatrix}\n",
    "6.50985010e-01 & -4.40168516e-02 & 3.65903823e+02\\\\\n",
    "-7.99527543e-02 & 8.72003506e-01 & -1.59370556e+01\\\\\n",
    "-3.55709534e-04 & -1.73167455e-05 & 1.00000000e+00\\\\\n",
    " \\end{bmatrix}$$\n",
    "\n",
    "Image1\n",
    "<img src='data/incline_L.png'/>\n",
    "\n",
    "Image2\n",
    "<img src='data/incline_L.png'/>\n",
    "\n",
    "Stitched Image\n",
    "<img src='results/q4_1.png'/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-metadata",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "670353343ebe97eeea49edc51d4e2c02",
     "grade": false,
     "grade_id": "q4_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2 Image Stitching with No Clip (3 pts)\n",
    "\n",
    "Visualize the warped image. Please include the image in your writeup PDF, along with stating which image pair you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af604480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageStitching_noClip(im1,\n",
    "                          im2,\n",
    "                          H2to1,\n",
    "                          pad=500,\n",
    "                          isPadWidth=True,\n",
    "                          M=np.array([[0.8, 0, 50],\n",
    "                                      [0, 0.8, 200],\n",
    "                                      [0, 0, 1.]])):\n",
    "    '''\n",
    "    Returns a panorama of im1 and im2 using the given \n",
    "    homography matrix without cliping.\n",
    "    \n",
    "    INPUTS\n",
    "        im1 and im2 - images to be stitched.\n",
    "        H2to1- the homography matrix.\n",
    "        pad - Number of pixels to increase width by. Default 500\n",
    "        M - scaling and translation matrix.\n",
    "    OUTPUT\n",
    "        img_pano - the panorama image.\n",
    "    ''' \n",
    "    \n",
    "    if isPadWidth:\n",
    "        w, h = im1.shape[1] + pad, (im1.shape[0] * (im1.shape[1] + pad)) // im1.shape[1]\n",
    "    else:\n",
    "        w, h = (im1.shape[1] * (im1.shape[0] + pad)) // im1.shape[0], im1.shape[0] + pad\n",
    "    \n",
    "    out_size = (w, h)\n",
    "    \n",
    "    warp_im1 = cv2.warpPerspective(im1, M, out_size)\n",
    "    warp_im2 = cv2.warpPerspective(im2, M @ H2to1, out_size)\n",
    "        \n",
    "    # Generating weights for alpha blending.\n",
    "    w1 = distance_transform_edt(np.pad(np.ones_like(im1[1: -1, 1: -1, 0]), 1))\n",
    "    w2 = distance_transform_edt(np.pad(np.ones_like(im2[1: -1, 1: -1, 0]), 1))\n",
    "    \n",
    "    # Warping weights for im1 to the target shape.\n",
    "    warp_w1 = cv2.warpPerspective(w1, M, out_size)\n",
    "    warp_w1 = warp_w1.reshape(warp_w1.shape[0], -1, 1)\n",
    "    \n",
    "    # Warping weights for im2 to the target shape.\n",
    "    warp_w2 = cv2.warpPerspective(w2, M @ H2to1, out_size)\n",
    "    warp_w2 = warp_w2.reshape(warp_w2.shape[0], -1, 1)\n",
    "    \n",
    "    img_pano = np.uint8((warp_im1 * warp_w1 + warp_im2 * warp_w2) / (warp_w1 + warp_w2))\n",
    "    \n",
    "    return img_pano\n",
    "\n",
    "im1 = cv2.cvtColor(cv2.imread('data/incline_L.png'), cv2.COLOR_BGR2RGB)\n",
    "im2 = cv2.cvtColor(cv2.imread('data/incline_R.png'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "locs1, desc1 = briefLite(im1)\n",
    "locs2, desc2 = briefLite(im2)\n",
    "matches = briefMatch(desc1, desc2)\n",
    "\n",
    "H, inliers = computeH_ransac(matches, locs1, locs2)\n",
    "print('H: {0}'.format(H / H[2, 2]))\n",
    "\n",
    "img_pano = imageStitching_noClip(im1, im2, H)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.imshow(img_pano)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd83af-6c3d-48b6-a70e-a7bacb7a9256",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "Image1\n",
    "<img src='data/incline_L.png'/>\n",
    "\n",
    "Image2\n",
    "<img src='data/incline_L.png'/>\n",
    "\n",
    "Stitched Image\n",
    "<img src='results/q4_2.png'/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-presentation",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a942b6b65d616036124aa6c1c5f792b0",
     "grade": false,
     "grade_id": "q4_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.3 Generate Panorama (2 pts)\n",
    "\n",
    "Save the resulting panorama on the full sized images and include the figure and computed homography matrix in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePanorama(im1, im2):\n",
    "    '''\n",
    "    Gnerate a panorama from two images.\n",
    "    \n",
    "    INPUTS\n",
    "        im1 and im2 - images to be stitched.\n",
    "    OUTPUT\n",
    "        img_pano - the panorama image.\n",
    "    '''\n",
    "    im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2RGB)\n",
    "    im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    locs1, desc1 = briefLite(im1)\n",
    "    locs2, desc2 = briefLite(im2)\n",
    "    matches = briefMatch(desc1, desc2)\n",
    "    H, inliers = computeH_ransac(matches, locs1, locs2)\n",
    "    print('H: {0}'.format(H / H[2, 2]))\n",
    "\n",
    "\n",
    "    img_pano = imageStitching_noClip(im1, im2, H)\n",
    "    \n",
    "    return img_pano\n",
    "\n",
    "im1 = cv2.imread('data/incline_L.png')\n",
    "im2 = cv2.imread('data/incline_R.png')\n",
    "\n",
    "img_pano = generatePanorama(im1, im2)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.imshow(img_pano)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92a110-0685-48a7-9a3f-30be8203a100",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "$$H2to1= \\begin{bmatrix}\n",
    "6.70688040e-01 & -3.24766846e-02 & 3.61588728e+02\\\\\n",
    "-7.62290199e-02 & 8.90118506e-01 & -2.09211372e+01\\\\\n",
    "-3.42501786e-04 & -2.92305590e-06 & 1.00000000e+00\\\\\n",
    " \\end{bmatrix}$$\n",
    "\n",
    "Panorama\n",
    "<img src='results/q4_3.png'/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-grant",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61f3929731831a39d495887a208c2772",
     "grade": false,
     "grade_id": "q4_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.4 extra credits (3 pts)\n",
    "\n",
    "Collect a pair of your own images (with your phone) and stitch them together using your code from the previous section. Include the pair of images and their result in the write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = cv2.imread('data/custom_l.jpeg')\n",
    "im2 = cv2.imread('data/custom_r.jpeg')\n",
    "\n",
    "im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2RGB)\n",
    "im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "locs1, desc1 = briefLite(im1)\n",
    "locs2, desc2 = briefLite(im2)\n",
    "matches = briefMatch(desc1, desc2)\n",
    "\n",
    "# plotMatches(im1, im2, matches, locs1, locs2)\n",
    "\n",
    "H, inliers = computeH_ransac(matches, locs1, locs2)\n",
    "print('H: {0}'.format(H / H[2, 2]))\n",
    "\n",
    "M = np.array([[0.6, 0, 0],\n",
    "              [0, 0.6, 400],\n",
    "              [0, 0, 1.]])\n",
    "\n",
    "img_pano = imageStitching_noClip(im1, im2, H, pad=800, isPadWidth=False, M=M)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 15))\n",
    "\n",
    "axes[0].imshow(im1)\n",
    "axes[0].set_title('Im1')\n",
    "axes[1].imshow(im2)\n",
    "axes[1].set_title('Im2')\n",
    "axes[2].imshow(img_pano)\n",
    "axes[2].set_title('Pano No Clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bdedd-bbf4-4f11-a614-83f18b179b8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "<img src='results/q4_4.png'/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-yacht",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9be81bee3ac8380c1c5d4d563bc5eb2",
     "grade": false,
     "grade_id": "q4_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 4.5 extra credits (2 pts)\n",
    "\n",
    "Collect at least 6 images  and stitch them into a single noClip image. You can either collect your own, or use the [PNC Park images](http://www.cs.jhu.edu/~misha/Code/SMG/PNC3.zip) from Matt Uyttendaele. We used the PNC park images (subsmapled to 1/4 sized) and ORB keypoints and descriptors for our reference solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d343c-22b1-4a49-8200-fa5beada609c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "YOUR ANSWER HERE...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-falls",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6131a12e81b99127f4e25c5d4fb6a4b9",
     "grade": false,
     "grade_id": "q5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 5: Poisson Image Stitching (15 points)\n",
    "\n",
    "Write a function called $\\texttt{poisson_blend(background,foreground,mask)}$ which takes 3 equal sized images (background and foreground as RGB, mask as binary) and solves the Poisson equation, using gradients from foreground and boundary conditions from the background. \n",
    "\n",
    "**The problem will be manually graded.** Please include results from both the $\\texttt{(fg1,bg1,mask1)}$ and $\\texttt{(fg2,bg2,mask2)}$ images in your write-up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e3e62",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_border(mask, x, y, c):\n",
    "    \n",
    "    if (mask[x - 1, y, c] == 0) or (mask[x + 1, y, c] == 0) or \\\n",
    "        (mask[x, y - 1, c] == 0) or (mask[x, y + 1, c] == 0):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def poisson_blend(bg, fg, mask):\n",
    "    \n",
    "    blended_img = np.uint8(np.uint8(mask == 0) * bg + np.uint8(mask != 0) * fg)\n",
    "    plt.imshow(blended_img)\n",
    "    plt.show()\n",
    "    \n",
    "    mask_nzx, mask_nzy = np.nonzero(mask[:, :, 0])    \n",
    "    mask_nz = [(int(x), int(y)) for x, y in zip(mask_nzx, mask_nzy)]\n",
    "    N = len(mask_nz)\n",
    "    \n",
    "    A = np.eye(N, dtype=np.float16) * 4\n",
    "    \n",
    "    # Iterate through every non-zero pixel and fill A.\n",
    "    for p in range(N):\n",
    "        \n",
    "        points = [(mask_nz[p][0] - 1, mask_nz[p][1]),\n",
    "                  (mask_nz[p][0] + 1, mask_nz[p][1]), \n",
    "                  (mask_nz[p][0], mask_nz[p][1] - 1),\n",
    "                  (mask_nz[p][0], mask_nz[p][1] + 1)]\n",
    "        \n",
    "        for x, y in points:            \n",
    "            if (x, y) not in mask_nz:\n",
    "                continue\n",
    "            \n",
    "            A[p, mask_nz.index((x, y))] = -1\n",
    "    \n",
    "    # Solve Ax=b for each channel.\n",
    "    for c in range(blended_img.shape[-1]):\n",
    "        b = np.zeros(N)\n",
    "        \n",
    "        for j in range(N): \n",
    "            b[j] = 4 * fg[mask_nz[j][0], mask_nz[j][1], c] \\\n",
    "                - fg[mask_nz[j][0] - 1, mask_nz[j][1], c] \\\n",
    "                - fg[mask_nz[j][0] + 1, mask_nz[j][1], c] \\\n",
    "                - fg[mask_nz[j][0], mask_nz[j][1] - 1, c] \\\n",
    "                - fg[mask_nz[j][0], mask_nz[j][1] + 1, c]\n",
    "            \n",
    "            if (is_border(mask, mask_nz[j][0], mask_nz[j][1], c)):\n",
    "                if (mask[mask_nz[j][0] - 1, mask_nz[j][1], c] == 0):\n",
    "                    b[j] += bg[mask_nz[j][0] - 1, mask_nz[j][1], c]\n",
    "                else:\n",
    "                    b[j] += fg[mask_nz[j][0] - 1, mask_nz[j][1], c]\n",
    "\n",
    "                if (mask[mask_nz[j][0] + 1, mask_nz[j][1], c] == 0):\n",
    "                    b[j] += bg[mask_nz[j][0] + 1, mask_nz[j][1], c]\n",
    "                else:\n",
    "                    b[j] += fg[mask_nz[j][0] + 1, mask_nz[j][1], c]\n",
    "\n",
    "                if (mask[mask_nz[j][0], mask_nz[j][1] - 1, c] == 0):\n",
    "                    b[j] += bg[mask_nz[j][0], mask_nz[j][1] - 1, c]\n",
    "                else:\n",
    "                    b[j] += fg[mask_nz[j][0], mask_nz[j][1] - 1, c]\n",
    "\n",
    "                if (mask[mask_nz[j][0], mask_nz[j][1] + 1, c] == 0):\n",
    "                    b[j] += bg[mask_nz[j][0], mask_nz[j][1] + 1, c]\n",
    "                else:\n",
    "                    b[j] += fg[mask_nz[j][0], mask_nz[j][1] + 1, c]\n",
    "    \n",
    "        i = np.linalg.solve(A, b)\n",
    "        i = (i - np.min(i)) * 255. / (np.max(i) - np.min(i))\n",
    "        blended_img[mask_nzx, mask_nzy, c] = i\n",
    "    \n",
    "    return np.uint8(blended_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6566ea-87da-44c3-a02e-d605e51a2bce",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "FG:\n",
    "<img src='data/fg1.png'>\n",
    "\n",
    "BG:\n",
    "<img src='data/bg1.png'>\n",
    "\n",
    "Mask:\n",
    "<img src='data/mask1.png'>\n",
    "\n",
    "Result:\n",
    "<img src='results/q5.png'>\n",
    "\n",
    "While the resulting image does not look right, I feel the algorith implemented is right. So, if you could help figure out what I am doing wrong, that would help a lot.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3262b3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
