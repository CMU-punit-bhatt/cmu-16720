{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed6079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c8ee7d1",
   "metadata": {},
   "source": [
    "![img](figures/course.png)\n",
    "\n",
    "#                                    16720 (B) Bag of Visual Words - Assignment 2\n",
    "\n",
    "     Instructor: Kris Kitani                       TAs:Paritosh (Lead), Rawal, Yan, Zen, Wen-Hsuan, Qichen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7788918",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ba35ab89cdc3e39c5704e8e4e387627",
     "grade": false,
     "grade_id": "cell-2f0bf8de83a87eae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Theory Questions\n",
    "\n",
    "This section should include the visualizations and answers to specifically highlighted questions from P1 to P4. This section will be manually Graded "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f0eb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "199f0b423bb79cdeb8d761ce8f9df98d",
     "grade": false,
     "grade_id": "cell-d2e7501d5ec1729e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Q1.1.1 (5 Points WriteUp)\n",
    "What visual properties do each of the filter functions (See Figure below) pick up? You should group the filters into categories by its purpose/functionality. Also, why do we need multiple scales of filter responses? **Answer in the writeup. Answer in your write-up.**\n",
    "\n",
    "<img align=\"center\" src=\"figures/filters_image.png\" width=\"500\">\n",
    "<figcaption align=\"center\"><b>Figure1. The provided multi-scale filter bank</b></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed067d",
   "metadata": {},
   "source": [
    "![img](figures/filters_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ab82c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "deca7d19a5637c50906e02a8d6c4877f",
     "grade": true,
     "grade_id": "cell-f20eebb8abbd872b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "There are four types of filters used.<br>\n",
    "1 - Gaussian filters - These are low-pass filters that help get rid of the high-frequency components and thus, end up having a smoothening/blurring effect on an image.<br>\n",
    "2 - Laplacian filters - These help detect edges. These produce a peak at start of the change in intensity and then at the end of the change.<br>\n",
    "3 - Derivative of Gaussian in x - These help detect intensity changes in the x direction. Thus, help picking up vertical edges.<br>\n",
    "4 - Derivative of Gaussian in y - These help detect intensity changes in the y direction. Thus, help picking up horizontal edges.<br><br>\n",
    "Based on the above, the laplacian and derivative filters kind of play a similar role in terms of picking up sharp changes while gaussian is slightly different as it subdues high frequency and noise.<br>\n",
    "As the scale increase, it essentially, in this context, means that the sigma/variance or spread of the filter increases. It is equivalent in practice, to the same filter being applied on a sub-sampled image. With this, the filters would be able to capture visual properties at varied scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda6631",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90b6856c73e0ec7654f58f2bfa2e8d7f",
     "grade": false,
     "grade_id": "cell-f8136fffb67fc66f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q1.3.1 (5 Points WriteUp)\n",
    "\n",
    "Visualize three wordmaps of images from any one of the category. **Include these in your write-up, along with the original RGB images. Include some comments on these visualizations: do the “word” boundaries make sense to you?**. We have provided helper function to save and visualize the resulting wordmap in the util.py file. They should look similar to the ones in Figure 2.\n",
    "\n",
    "<img align=\"center\" src=\"./figures/textons.jpg\" width=\"800\">\n",
    "<figcaption align = \"center\"><b>Figure 2. Visual words over images. You will use the spatially un-ordered distribution of visual words in a region (a bag of visual words) as a feature for scene classification, with some coarse information provided by spatial pyramid matching [2]</b></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b81d5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0337fe508756898a619bf94bf9c0fc60",
     "grade": true,
     "grade_id": "cell-806d8af4e95d61d2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![image](results/plot_1_3_0_.png)\n",
    "![image](results/plot_1_3_1_.png)\n",
    "![image](results/plot_1_3_2_.png)\n",
    "<br>\n",
    "Image pixels with similar intensity and gradients have similar features and thus, all are associated with the same dictionary \"word\". As the pixel intensities vary, the features around those pixels differ as well, leading to different dictionary words in those regions. This can be seen in the above images as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6b431",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "456c33594131e9a50908bfcc7b703f4e",
     "grade": false,
     "grade_id": "cell-2cf410e4507cf87f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q2.1\n",
    "**For 5 Images, include their visual word maps and histograms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b6192",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aa3b80c369eccfad7ee7aa4509d0770",
     "grade": true,
     "grade_id": "cell-f8873a304123ee24",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![image](results/plot_2_1_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e32d84",
   "metadata": {},
   "source": [
    "![image](results/plot_2_1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f82f64",
   "metadata": {},
   "source": [
    "![image](results/plot_2_1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a201d8f",
   "metadata": {},
   "source": [
    "![image](results/plot_2_1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5813b",
   "metadata": {},
   "source": [
    "![image](results/plot_2_1_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b53b24",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd2391f7ef74f5d6be7b06374f7d41de",
     "grade": false,
     "grade_id": "cell-f11c4f53168fabbf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q3.1.1 \n",
    "Submit the visualization of Confusion Matrix and the Accuracy value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e901a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "189f94a62bb1a83b1fb8a933028e5306",
     "grade": true,
     "grade_id": "cell-a67d219e82ac3ea5",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Confusion Matrix - <br>\n",
    "\\begin{verbatim}\n",
    "[[14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "[ 1., 12.,  1.,  1.,  0.,  1.,  2.,  0.],\n",
    "[ 0.,  0., 16.,  3.,  2.,  1.,  0.,  3.],\n",
    "[ 0.,  1.,  2., 13.,  0.,  1.,  3.,  6.],\n",
    "[ 2.,  0.,  1.,  0.,  9.,  1.,  0.,  0.],\n",
    "[ 1.,  2.,  0.,  1.,  8., 10.,  2.,  0.],\n",
    "[ 2.,  2.,  1.,  1.,  2.,  0., 13.,  0.],\n",
    "[ 0.,  1.,  2.,  3.,  2.,  0.,  1., 10.]]\n",
    "\\end{verbatim}\n",
    "<br>\n",
    "![conf_mat](results/conf_mat_3_1.png)\n",
    "\n",
    "Accuracy - 60.625%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58420900",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5219c1bfeff05a50e6c52fb438b4f120",
     "grade": false,
     "grade_id": "cell-c77fa30dd0533616",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Q3.1.2 (5 points WriteUp):\n",
    "\n",
    "As there are some classes/samples that are more difficult to classify than the rest using the bags-of-words approach, they are more easily classified incorrectly into other categories. **List some of these classes/samples and discuss why they are more difficult.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0af44b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aeaa24fd50837bd0204227238782e828",
     "grade": true,
     "grade_id": "cell-fe8e3fd47e21e13c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "As can be seen from the above confusion matrix, there are instances where images with true label 3 (highway) are being classified as class 7 (windmill). Similarly, images with true label 5 (laundromat) are being classified as class 4 (kitchen). <br>\n",
    "These classes have quite a bit of similarity - 3,7 are both outdoors with blue skies, 2 lines (highway road, windmill); 4, 5 are both indoors with furniture, counter tops etc. Due to such similarities, it would be hard to distinguish between them, in some cases, based on BoW. Because of such similarities, a test image would end up having high word count for certain features which would be true for training examples of the other class thus, leading to misclassification.\n",
    "<br>\n",
    "![img](results/eg1.jpg)\n",
    "<br>\n",
    "For example, the above laundromat image was misclassified as kitchen. A lot of the kitchen images have similar wood/brown color components and so, the corresponding wordmaps/histogram of this image would have had high similarity with those images leading to the misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4e800",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2da804b4ebd6680ded96429b206041e1",
     "grade": false,
     "grade_id": "cell-a0d4cf029c9816a6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q3.1.3 [Extra Credit](10 points) Manually Graded:\n",
    "\n",
    "Now that you have seen how well your recognition system can perform on a set of real images, you can experiment with different ways of improving this baseline system. \n",
    "\n",
    "Include the changes, modification you made and the impact it had on accuracy.\n",
    "\n",
    "Tune the system you build to reach around 65\\% accuracy on the provided test set (``data/test_data.npz``). **In your writeup, document what you did to achieve such performance: (1) what you did, (2) what you expected would happen, and (3) what actually happened.** Also, include a file called ``custom.py/ipynb`` for running your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d7f31",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b04b30519c4d03f7064609fb83fb0d7",
     "grade": true,
     "grade_id": "cell-b7979e73bac0c915",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3da0b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66483f7e50480fa8a43693f18bd54e74",
     "grade": false,
     "grade_id": "cell-0ab5de6e6222b473",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q3.1.4 [Extra Credit](10 points):\n",
    "**Inverse Document Frequency:** With the bag-of-word model, image recognition is similar to classifying a document with words. In document classification, inverse document frequency (IDF) factor is incorporated which diminishes the weight of terms that occur very frequently in the document set. For example, because the term \"the\" is so common, this will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms.\n",
    "\n",
    "In the homework, the histogram we computed only considers the term frequency (TF), i.e.  the number of times that word occurs in the word map.  Now we want to weight the word by its inverse document frequency.  The IDF of a word is defined as:\n",
    "\n",
    "\\begin{align*} IDF_w &= log \\frac{T}{|\\{d: w \\in\n",
    "d\\}|}\\\\ \\end{align*}\n",
    "\n",
    "Here, $T$ is number of all training images, and $|\\{d:w\\in d\\}|$ is the number of images $d$ such that $w$ occurs in that image.\n",
    "\n",
    "**In your writeup: How does Inverse Document Frequency affect the performance? Better or worse? Explain your reasoning?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_idf(args):\n",
    "    file_path, dictionary, layer_num, K, trained_features, train_labels, idf = args\n",
    "    \n",
    "    n_patches = int(trained_features.shape[1] / K)\n",
    "    \n",
    "    # this will tile it 21 times as count columns and then flatten it to match the features.\n",
    "    idf_hist_all = np.repeat(idf.reshape(-1, 1), n_patches).flatten()\n",
    "    \n",
    "    _, feature = get_image_feature(file_path, dictionary, layer_num, K)\n",
    "    distances = distance_to_set(feature * idf_hist_all, trained_features * idf_hist_all)\n",
    "    pred_label = train_labels[np.argmax(distances)]   \n",
    "    \n",
    "    return pred_label  \n",
    "\n",
    "def compute_IDF():\n",
    "    trained_system = np.load(\"trained_system.npz\")\n",
    "    trained_features = trained_system['features']\n",
    "    dictionary = trained_system['dictionary']\n",
    "    \n",
    "    K = dictionary.shape[0]\n",
    "    T = trained_features.shape[0]\n",
    "    \n",
    "    word_count = np.sum(trained_features != 0, axis = 0)\n",
    "    idf = np.zeros((K))\n",
    "    n_patches = int(word_count.shape[0] / K)\n",
    "    j = 0\n",
    "    \n",
    "    idf_hist_all = np.log(T / word_count)\n",
    "    \n",
    "    for i in range(0, word_count.shape[0], n_patches):\n",
    "        idf[j] = word_count[i]\n",
    "        j += 1\n",
    "    \n",
    "    assert idf.shape[0] == K\n",
    "    np.save('idf.npy', idf)\n",
    "    \n",
    "    return idf\n",
    "\n",
    "def evaluate_recognition_System_IDF():\n",
    "    test_data = np.load(\"./data/test_data.npz\")\n",
    "    trained_system = np.load(\"trained_system.npz\")\n",
    "    \n",
    "    image_names = test_data['files']\n",
    "    test_labels = test_data['labels']\n",
    "\n",
    "    trained_features = trained_system['features']\n",
    "    train_labels = trained_system['labels']\n",
    "    \n",
    "    dictionary = trained_system['dictionary']\n",
    "    SPM_layer_num = trained_system['SPM_layer_num']\n",
    "    SPM_layer_num = int(SPM_layer_num)\n",
    "    K = dictionary.shape[0]\n",
    "\n",
    "    print(\"Trained features shape: \", trained_features.shape)\n",
    "    print(\"Test data shape: \", image_names.shape)\n",
    "\n",
    "    idf = compute_IDF()\n",
    "    \n",
    "    num_images = len(image_names)\n",
    "    ordered_labels = []\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        full_image_name = './data/' + image_names[i]\n",
    "        ordered_labels.append(evaluate_single_idf([full_image_name,\n",
    "                                                   dictionary,\n",
    "                                                   3,\n",
    "                                                   K,\n",
    "                                                   trained_features,\n",
    "                                                   train_labels,\n",
    "                                                   idf.reshape(1, -1)]))\n",
    "        \n",
    "    \n",
    "    ordered_labels = np.array(ordered_labels, dtype=int)\n",
    "    assert ordered_labels.shape[0] == test_labels.shape[0]    \n",
    "    \n",
    "    conf_matrix = np.zeros((8, 8))\n",
    "    \n",
    "    for i in range(ordered_labels.shape[0]):\n",
    "        conf_matrix[test_labels[i], ordered_labels[i]] += 1\n",
    "        \n",
    "    accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)    \n",
    "    np.save(\"./conf_matrix.npy\", conf_matrix)\n",
    "    \n",
    "    return conf_matrix, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01349c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91cebc6bce44b037e5405cebd599c2bb",
     "grade": true,
     "grade_id": "cell-8949e75ea938cd42",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Confusion Matrix with IDF - <br>\n",
    "\n",
    "\\begin{verbatim}\n",
    "[[14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "[ 0., 12.,  1.,  2.,  0.,  1.,  2.,  0.],\n",
    "[ 0.,  1., 12.,  4.,  1.,  4.,  1.,  2.],\n",
    "[ 0.,  3.,  2., 13.,  0.,  1.,  3.,  4.],\n",
    "[ 2.,  0.,  1.,  0.,  9.,  1.,  0.,  0.],\n",
    "[ 2.,  0.,  0.,  2.,  7., 11.,  2.,  0.],\n",
    "[ 1.,  1.,  0.,  0.,  2.,  2., 15.,  0.],\n",
    "[ 0.,  0.,  1.,  6.,  2.,  0.,  0., 10.]]\n",
    "\\end{verbatim}\n",
    "![conf_mat](results/conf_mat_3_3_1.png)\n",
    "\n",
    "<br>\n",
    "Accuracy - 60%\n",
    "\n",
    "<br>\n",
    "IDF led to decrease in performance. IDF, ideally, helps deprioritize features that occur across a large number of images as they wouldn't be a good factor at actually differentiating images. However, these weights are learned over the training images and so, these generated weights are overfitted on these images. Hence, this heuristic wont help categorizing the test images better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d934de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28f9ab217c686b3e1a3c3aa90c5050a5",
     "grade": false,
     "grade_id": "cell-5a254c9a47e7f561",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q4.2.1 \n",
    "**Report the confusion matrix and accuracy for your results in your write-up. Can you comment in your writeup on whether the results are better or worse than classical BoW - why do you think that is?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece79aea",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12da5faa2c3fc2315a366648c12b87d4",
     "grade": true,
     "grade_id": "cell-c383f7a8536d254d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Confusion Matrix - <br>\n",
    "\n",
    "\\begin{verbatim}\n",
    "[[14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "[ 0., 17.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
    "[ 0.,  0., 24.,  0.,  0.,  0.,  0.,  1.],\n",
    "[ 0.,  0.,  0., 26.,  0.,  0.,  0.,  0.],\n",
    "[ 0.,  0.,  0.,  0., 12.,  1.,  0.,  0.],\n",
    "[ 0.,  0.,  0.,  0.,  1., 23.,  0.,  0.],\n",
    "[ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.],\n",
    "[ 0.,  0.,  0.,  0.,  0.,  0.,  0., 19.]]\n",
    "\\end{verbatim}\n",
    "\n",
    "![conf_mat](results/conf_mat_4_2.png)\n",
    "<br>\n",
    "Accuracy - 97.5 %\n",
    "\n",
    "The VGG16 performs much better with an accuracy of 97.5% as compared to the 60.625% of the BoW. I believe this is because the network has learnt effective features for distinguishing on its own rather than the 4 types of features we used with BoW. In case of BoW, we rely on the relation of the frequency of low level features (responses to gaussian, laplacian, DoG in x and y) around keypoints occuring in the image. These can occur in all classes and so, might not be the most reliable when it comes to classifying. However, the network learns these low level features but with every deeper layer, it combines these to get a sense of higher level features as well. Thus, it is able to pick up complex understanding of the scene image and can classify it better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b277ef6",
   "metadata": {},
   "source": [
    "### Q1.1.2 Manually grade the image\n",
    "Can use sun_aztvjgubyrgvirup.jpg for visualizing the 20 image collage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb3d1b",
   "metadata": {},
   "source": [
    "![collage](results/plot_1_1_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65de637",
   "metadata": {},
   "source": [
    "### Q1.2.1 Manually grade 5 images for Harris Corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749eea33",
   "metadata": {},
   "source": [
    "![img1](results/plot_1_2_0.png)\n",
    "![img1](results/plot_1_2_1.png)\n",
    "![img1](results/plot_1_2_2.png)\n",
    "![img1](results/plot_1_2_3.png)\n",
    "![img1](results/plot_1_2_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060890e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff4e09d015af0ae2a101a7f03fcc8a9",
     "grade": false,
     "grade_id": "cell-39235682903e017c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### References\n",
    "\n",
    "[1]  James Hays and Alexei A Efros. Scene completion using millions of photographs.ACM Transactions onGraphics (SIGGRAPH 2007), 26(3), 2007.\n",
    "\n",
    "[2]  S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recogniz-ing natural scene categories.  InComputer Vision and Pattern Recognition (CVPR), 2006 IEEE Conferenceon, volume 2, pages 2169–2178, 2006.\n",
    "\n",
    "[3]  Jian xiong Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recogni-tion from abbey to zoo.2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,pages 3485–3492, 2010.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027ffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
